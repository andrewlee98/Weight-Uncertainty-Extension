{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "from Bayes_By_Backprop.model import *\n",
    "from Bayes_By_Backprop_Local_Reparametrization.model import *\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 0.40M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n",
      "it 0/100, Jtr_KL = 0.866287, Jtr_pred = 0.892645, err = 0.281933, \u001b[31m   time: 22.390602 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.283794, err = 0.082300\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_KL = 0.809035, Jtr_pred = 0.422223, err = 0.125817, \u001b[31m   time: 7.845828 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223389, err = 0.064100\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_KL = 0.752716, Jtr_pred = 0.348886, err = 0.103883, \u001b[31m   time: 7.876497 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.190191, err = 0.056700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_KL = 0.704130, Jtr_pred = 0.306251, err = 0.090117, \u001b[31m   time: 7.655761 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.170901, err = 0.050000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_KL = 0.663028, Jtr_pred = 0.281959, err = 0.083383, \u001b[31m   time: 7.756209 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.155060, err = 0.046600\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_KL = 0.628508, Jtr_pred = 0.262554, err = 0.076433, \u001b[31m   time: 8.073391 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.143805, err = 0.042900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_KL = 0.599462, Jtr_pred = 0.248583, err = 0.072083, \u001b[31m   time: 8.195409 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.136788, err = 0.040500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_KL = 0.575624, Jtr_pred = 0.241487, err = 0.070983, \u001b[31m   time: 7.846514 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.128607, err = 0.039300\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_KL = 0.555980, Jtr_pred = 0.229304, err = 0.068483, \u001b[31m   time: 7.864027 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.121314, err = 0.036700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_KL = 0.539494, Jtr_pred = 0.226397, err = 0.066667, \u001b[31m   time: 7.898238 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.121300, err = 0.037000\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_KL = 0.526003, Jtr_pred = 0.218562, err = 0.063350, \u001b[31m   time: 7.996213 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.114953, err = 0.034600\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_KL = 0.514451, Jtr_pred = 0.213984, err = 0.063100, \u001b[31m   time: 7.698987 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.110982, err = 0.033000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_KL = 0.505075, Jtr_pred = 0.208264, err = 0.061383, \u001b[31m   time: 7.973331 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.111896, err = 0.034000\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_KL = 0.497337, Jtr_pred = 0.205352, err = 0.061400, \u001b[31m   time: 7.791916 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.107597, err = 0.032400\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_KL = 0.490816, Jtr_pred = 0.204303, err = 0.060783, \u001b[31m   time: 7.626509 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.106213, err = 0.032900\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_KL = 0.485425, Jtr_pred = 0.200543, err = 0.059550, \u001b[31m   time: 7.463617 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.103065, err = 0.030900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_KL = 0.480793, Jtr_pred = 0.196911, err = 0.058700, \u001b[31m   time: 7.856143 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.099600, err = 0.030800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_KL = 0.476864, Jtr_pred = 0.194209, err = 0.057667, \u001b[31m   time: 8.094342 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.100305, err = 0.031300\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_KL = 0.473461, Jtr_pred = 0.192409, err = 0.057883, \u001b[31m   time: 8.035843 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.098000, err = 0.029800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_KL = 0.470387, Jtr_pred = 0.191392, err = 0.056333, \u001b[31m   time: 7.760376 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.097596, err = 0.030000\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_KL = 0.467987, Jtr_pred = 0.188672, err = 0.056600, \u001b[31m   time: 8.055861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093636, err = 0.027700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_KL = 0.465732, Jtr_pred = 0.185323, err = 0.054483, \u001b[31m   time: 7.951187 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.095086, err = 0.029200\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_KL = 0.463605, Jtr_pred = 0.183893, err = 0.053933, \u001b[31m   time: 7.792593 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.092179, err = 0.026700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_KL = 0.462051, Jtr_pred = 0.184748, err = 0.054767, \u001b[31m   time: 7.899044 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093057, err = 0.027700\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_KL = 0.460494, Jtr_pred = 0.183565, err = 0.055150, \u001b[31m   time: 7.504257 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.092385, err = 0.026900\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_KL = 0.459436, Jtr_pred = 0.179565, err = 0.053250, \u001b[31m   time: 8.090879 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089888, err = 0.027100\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_KL = 0.457715, Jtr_pred = 0.179616, err = 0.052533, \u001b[31m   time: 7.476199 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090796, err = 0.026100\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_KL = 0.456476, Jtr_pred = 0.178065, err = 0.052200, \u001b[31m   time: 7.712317 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090125, err = 0.027100\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_KL = 0.455205, Jtr_pred = 0.177236, err = 0.053183, \u001b[31m   time: 7.609562 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088848, err = 0.025400\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_KL = 0.454630, Jtr_pred = 0.175785, err = 0.053450, \u001b[31m   time: 7.898952 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086135, err = 0.025400\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_KL = 0.453402, Jtr_pred = 0.174772, err = 0.051317, \u001b[31m   time: 8.105512 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086138, err = 0.025900\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_KL = 0.452501, Jtr_pred = 0.175250, err = 0.052533, \u001b[31m   time: 7.782482 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.087975, err = 0.025500\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_KL = 0.451451, Jtr_pred = 0.172396, err = 0.051083, \u001b[31m   time: 7.662480 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088884, err = 0.025400\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_KL = 0.450533, Jtr_pred = 0.173009, err = 0.052317, \u001b[31m   time: 7.548908 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089344, err = 0.026300\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_KL = 0.449646, Jtr_pred = 0.172258, err = 0.051600, \u001b[31m   time: 7.660638 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.085857, err = 0.025400\n",
      "\u001b[0m\n",
      "it 35/100, Jtr_KL = 0.448750, Jtr_pred = 0.171846, err = 0.051417, \u001b[31m   time: 7.987290 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086246, err = 0.024600\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 36/100, Jtr_KL = 0.447867, Jtr_pred = 0.169814, err = 0.051533, \u001b[31m   time: 7.957389 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083396, err = 0.024200\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 37/100, Jtr_KL = 0.447199, Jtr_pred = 0.170918, err = 0.050567, \u001b[31m   time: 7.443997 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.082856, err = 0.025100\n",
      "\u001b[0m\n",
      "it 38/100, Jtr_KL = 0.446491, Jtr_pred = 0.167750, err = 0.049650, \u001b[31m   time: 8.066604 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.085716, err = 0.024900\n",
      "\u001b[0m\n",
      "it 39/100, Jtr_KL = 0.445715, Jtr_pred = 0.166730, err = 0.050650, \u001b[31m   time: 7.584780 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.084715, err = 0.024500\n",
      "\u001b[0m\n",
      "it 40/100, Jtr_KL = 0.444708, Jtr_pred = 0.169353, err = 0.050233, \u001b[31m   time: 7.560914 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.082658, err = 0.023700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 41/100, Jtr_KL = 0.444035, Jtr_pred = 0.167359, err = 0.050317, \u001b[31m   time: 7.447401 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083369, err = 0.024100\n",
      "\u001b[0m\n",
      "it 42/100, Jtr_KL = 0.443384, Jtr_pred = 0.166849, err = 0.049683, \u001b[31m   time: 7.719210 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.081408, err = 0.024300\n",
      "\u001b[0m\n",
      "it 43/100, Jtr_KL = 0.442584, Jtr_pred = 0.165027, err = 0.050550, \u001b[31m   time: 7.888446 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083943, err = 0.024900\n",
      "\u001b[0m\n",
      "it 44/100, Jtr_KL = 0.441574, Jtr_pred = 0.165552, err = 0.048833, \u001b[31m   time: 7.967153 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080121, err = 0.024400\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 45/100, Jtr_KL = 0.440858, Jtr_pred = 0.164680, err = 0.049500, \u001b[31m   time: 7.692169 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080781, err = 0.023100\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 46/100, Jtr_KL = 0.440383, Jtr_pred = 0.163989, err = 0.050483, \u001b[31m   time: 7.618830 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.084430, err = 0.024000\n",
      "\u001b[0m\n",
      "it 47/100, Jtr_KL = 0.439542, Jtr_pred = 0.165321, err = 0.050567, \u001b[31m   time: 7.631716 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083488, err = 0.024300\n",
      "\u001b[0m\n",
      "it 48/100, Jtr_KL = 0.438728, Jtr_pred = 0.162528, err = 0.048800, \u001b[31m   time: 7.906136 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080355, err = 0.024300\n",
      "\u001b[0m\n",
      "it 49/100, Jtr_KL = 0.437912, Jtr_pred = 0.163304, err = 0.050083, \u001b[31m   time: 7.936665 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083297, err = 0.024800\n",
      "\u001b[0m\n",
      "it 50/100, Jtr_KL = 0.437241, Jtr_pred = 0.161772, err = 0.048100, \u001b[31m   time: 7.618639 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083754, err = 0.025000\n",
      "\u001b[0m\n",
      "it 51/100, Jtr_KL = 0.436194, Jtr_pred = 0.161650, err = 0.048400, \u001b[31m   time: 7.774513 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080940, err = 0.024000\n",
      "\u001b[0m\n",
      "it 52/100, Jtr_KL = 0.435103, Jtr_pred = 0.163617, err = 0.049717, \u001b[31m   time: 7.488868 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.081887, err = 0.024600\n",
      "\u001b[0m\n",
      "it 53/100, Jtr_KL = 0.434453, Jtr_pred = 0.161117, err = 0.047883, \u001b[31m   time: 7.503868 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.081853, err = 0.024500\n",
      "\u001b[0m\n",
      "it 54/100, Jtr_KL = 0.433942, Jtr_pred = 0.159700, err = 0.047433, \u001b[31m   time: 7.953079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.081079, err = 0.024600\n",
      "\u001b[0m\n",
      "it 55/100, Jtr_KL = 0.433263, Jtr_pred = 0.162922, err = 0.049867, \u001b[31m   time: 7.953244 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079986, err = 0.023100\n",
      "\u001b[0m\n",
      "it 56/100, Jtr_KL = 0.432551, Jtr_pred = 0.159487, err = 0.047817, \u001b[31m   time: 7.827899 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079337, err = 0.023000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 57/100, Jtr_KL = 0.431976, Jtr_pred = 0.160817, err = 0.048100, \u001b[31m   time: 7.858419 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.082345, err = 0.025200\n",
      "\u001b[0m\n",
      "it 58/100, Jtr_KL = 0.431081, Jtr_pred = 0.159509, err = 0.047733, \u001b[31m   time: 7.940655 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080244, err = 0.023800\n",
      "\u001b[0m\n",
      "it 59/100, Jtr_KL = 0.430298, Jtr_pred = 0.160328, err = 0.048883, \u001b[31m   time: 8.123359 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.081458, err = 0.024200\n",
      "\u001b[0m\n",
      "it 60/100, Jtr_KL = 0.429616, Jtr_pred = 0.159720, err = 0.048083, \u001b[31m   time: 7.940921 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080842, err = 0.025000\n",
      "\u001b[0m\n",
      "it 61/100, Jtr_KL = 0.428923, Jtr_pred = 0.157819, err = 0.048383, \u001b[31m   time: 7.787081 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078943, err = 0.022900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 62/100, Jtr_KL = 0.428215, Jtr_pred = 0.159614, err = 0.047933, \u001b[31m   time: 8.111223 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079662, err = 0.024100\n",
      "\u001b[0m\n",
      "it 63/100, Jtr_KL = 0.427533, Jtr_pred = 0.158454, err = 0.049067, \u001b[31m   time: 7.553977 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.077194, err = 0.023300\n",
      "\u001b[0m\n",
      "it 64/100, Jtr_KL = 0.426589, Jtr_pred = 0.159967, err = 0.049017, \u001b[31m   time: 8.013518 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079491, err = 0.023600\n",
      "\u001b[0m\n",
      "it 65/100, Jtr_KL = 0.426215, Jtr_pred = 0.159082, err = 0.048567, \u001b[31m   time: 7.679069 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079024, err = 0.023800\n",
      "\u001b[0m\n",
      "it 66/100, Jtr_KL = 0.425524, Jtr_pred = 0.158295, err = 0.048800, \u001b[31m   time: 7.649812 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079727, err = 0.023000\n",
      "\u001b[0m\n",
      "it 67/100, Jtr_KL = 0.424896, Jtr_pred = 0.157862, err = 0.046767, \u001b[31m   time: 7.945591 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078298, err = 0.024300\n",
      "\u001b[0m\n",
      "it 68/100, Jtr_KL = 0.424199, Jtr_pred = 0.156789, err = 0.046967, \u001b[31m   time: 7.786473 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079907, err = 0.023300\n",
      "\u001b[0m\n",
      "it 69/100, Jtr_KL = 0.423485, Jtr_pred = 0.158762, err = 0.048550, \u001b[31m   time: 7.649971 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079103, err = 0.023000\n",
      "\u001b[0m\n",
      "it 70/100, Jtr_KL = 0.422720, Jtr_pred = 0.155955, err = 0.046850, \u001b[31m   time: 7.698879 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078860, err = 0.022800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 71/100, Jtr_KL = 0.422159, Jtr_pred = 0.156095, err = 0.047767, \u001b[31m   time: 7.545394 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079403, err = 0.023700\n",
      "\u001b[0m\n",
      "it 72/100, Jtr_KL = 0.421469, Jtr_pred = 0.155061, err = 0.047933, \u001b[31m   time: 7.763101 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078003, err = 0.023900\n",
      "\u001b[0m\n",
      "it 73/100, Jtr_KL = 0.420735, Jtr_pred = 0.154839, err = 0.047617, \u001b[31m   time: 7.600314 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080157, err = 0.022800\n",
      "\u001b[0m\n",
      "it 74/100, Jtr_KL = 0.419944, Jtr_pred = 0.155975, err = 0.047400, \u001b[31m   time: 7.605658 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079215, err = 0.023400\n",
      "\u001b[0m\n",
      "it 75/100, Jtr_KL = 0.419239, Jtr_pred = 0.156238, err = 0.046883, \u001b[31m   time: 7.618669 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.077147, err = 0.022700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 76/100, Jtr_KL = 0.418551, Jtr_pred = 0.154903, err = 0.047483, \u001b[31m   time: 7.793080 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.075509, err = 0.023200\n",
      "\u001b[0m\n",
      "it 77/100, Jtr_KL = 0.417936, Jtr_pred = 0.154369, err = 0.047167, \u001b[31m   time: 7.797060 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079856, err = 0.023800\n",
      "\u001b[0m\n",
      "it 78/100, Jtr_KL = 0.417231, Jtr_pred = 0.155851, err = 0.047133, \u001b[31m   time: 7.811295 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079664, err = 0.023700\n",
      "\u001b[0m\n",
      "it 79/100, Jtr_KL = 0.416479, Jtr_pred = 0.155184, err = 0.046483, \u001b[31m   time: 7.638900 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079231, err = 0.023800\n",
      "\u001b[0m\n",
      "it 80/100, Jtr_KL = 0.415876, Jtr_pred = 0.155542, err = 0.047350, \u001b[31m   time: 7.508023 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.077009, err = 0.022500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 81/100, Jtr_KL = 0.415060, Jtr_pred = 0.155279, err = 0.046567, \u001b[31m   time: 7.876943 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079216, err = 0.023400\n",
      "\u001b[0m\n",
      "it 82/100, Jtr_KL = 0.414353, Jtr_pred = 0.153643, err = 0.046883, \u001b[31m   time: 7.800621 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080062, err = 0.023300\n",
      "\u001b[0m\n",
      "it 83/100, Jtr_KL = 0.413561, Jtr_pred = 0.152524, err = 0.046100, \u001b[31m   time: 7.713956 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080983, err = 0.023800\n",
      "\u001b[0m\n",
      "it 84/100, Jtr_KL = 0.413106, Jtr_pred = 0.152869, err = 0.046500, \u001b[31m   time: 7.650783 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080536, err = 0.024500\n",
      "\u001b[0m\n",
      "it 85/100, Jtr_KL = 0.412569, Jtr_pred = 0.153598, err = 0.045983, \u001b[31m   time: 7.796293 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078595, err = 0.023000\n",
      "\u001b[0m\n",
      "it 86/100, Jtr_KL = 0.412080, Jtr_pred = 0.154479, err = 0.047183, \u001b[31m   time: 8.087940 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078829, err = 0.024500\n",
      "\u001b[0m\n",
      "it 87/100, Jtr_KL = 0.411116, Jtr_pred = 0.149901, err = 0.046483, \u001b[31m   time: 7.676305 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080115, err = 0.024100\n",
      "\u001b[0m\n",
      "it 88/100, Jtr_KL = 0.410485, Jtr_pred = 0.153679, err = 0.046400, \u001b[31m   time: 7.468417 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080425, err = 0.023700\n",
      "\u001b[0m\n",
      "it 89/100, Jtr_KL = 0.409911, Jtr_pred = 0.151733, err = 0.046367, \u001b[31m   time: 7.634501 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079520, err = 0.023700\n",
      "\u001b[0m\n",
      "it 90/100, Jtr_KL = 0.409271, Jtr_pred = 0.150607, err = 0.046017, \u001b[31m   time: 8.081030 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079778, err = 0.023300\n",
      "\u001b[0m\n",
      "it 91/100, Jtr_KL = 0.408726, Jtr_pred = 0.152232, err = 0.046417, \u001b[31m   time: 7.719512 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078250, err = 0.023800\n",
      "\u001b[0m\n",
      "it 92/100, Jtr_KL = 0.408288, Jtr_pred = 0.152098, err = 0.046100, \u001b[31m   time: 7.856463 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078939, err = 0.023900\n",
      "\u001b[0m\n",
      "it 93/100, Jtr_KL = 0.407701, Jtr_pred = 0.151387, err = 0.046633, \u001b[31m   time: 7.704722 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.077870, err = 0.022900\n",
      "\u001b[0m\n",
      "it 94/100, Jtr_KL = 0.407086, Jtr_pred = 0.152042, err = 0.045750, \u001b[31m   time: 7.678668 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078776, err = 0.023300\n",
      "\u001b[0m\n",
      "it 95/100, Jtr_KL = 0.406368, Jtr_pred = 0.152178, err = 0.046217, \u001b[31m   time: 7.815059 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.078324, err = 0.023000\n",
      "\u001b[0m\n",
      "it 96/100, Jtr_KL = 0.405756, Jtr_pred = 0.148817, err = 0.045317, \u001b[31m   time: 7.551616 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080196, err = 0.024000\n",
      "\u001b[0m\n",
      "it 97/100, Jtr_KL = 0.404781, Jtr_pred = 0.151158, err = 0.046567, \u001b[31m   time: 7.835059 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    Jdev = 0.080105, err = 0.024100\n",
      "\u001b[0m\n",
      "it 98/100, Jtr_KL = 0.404203, Jtr_pred = 0.151763, err = 0.046867, \u001b[31m   time: 8.146792 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.080607, err = 0.024500\n",
      "\u001b[0m\n",
      "it 99/100, Jtr_KL = 0.403849, Jtr_pred = 0.151438, err = 0.046233, \u001b[31m   time: 7.451503 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.079699, err = 0.023900\n",
      "\u001b[0m\n",
      "\u001b[31m   average time: 8.440922 seconds\n",
      "\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_last.dat\n",
      "\u001b[0m\n",
      "\u001b[36m\n",
      "RESULTS:\u001b[0m\n",
      "  cost_dev: 0.075509 (cost_train 0.148817)\n",
      "  err_dev: 0.022500\n",
      "  nb_parameters: 398420 (389.08KB)\n",
      "  time_per_it: 8.440922s\n",
      "\n",
      "snr: 199210\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_dir = 'bbb'\n",
    "results_dir = 'bbb'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "NTrainPointsMNIST = 60000\n",
    "batch_size = 128\n",
    "nb_epochs = 100\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-3\n",
    "nsamples = int(3)  # How many samples to estimate ELBO with at each iteration\n",
    "\n",
    "# if True: #args.model == 'Local_Reparam':\n",
    "net = BBP_Bayes_Net_LR(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n",
    "                 Nbatches=(NTrainPointsMNIST / batch_size), nhid=200, prior_sig=0.1)#args.prior_sig)\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "epoch = 0\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "    # We draw more samples on the first epoch in order to ensure convergence\n",
    "    if i == 0:\n",
    "        ELBO_samples = 10\n",
    "    else:\n",
    "        ELBO_samples = nsamples\n",
    "\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost_dkl, cost_pred, err = net.fit(x, y, samples=ELBO_samples)\n",
    "\n",
    "        err_train[i] += err\n",
    "        kl_cost_train[i] += cost_dkl\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    kl_cost_train[i] /= nb_samples  # Normalise by number of samples in order to get comparable number to the -log like\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_KL = %f, Jtr_pred = %f, err = %f, \" % (\n",
    "    i, nb_epochs, kl_cost_train[i], pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            cost, err, probs = net.eval(x, y)  # This takes the expected weights to save time, not proper inference\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir + '/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "net.save(models_dir + '/theta_last.dat')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "## Save results for plots\n",
    "# np.save('results/test_predictions.npy', test_predictions)\n",
    "np.save(results_dir + '/KL_cost_train.npy', kl_cost_train)\n",
    "np.save(results_dir + '/pred_cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "torch.save(net, 'bbb.net')\n",
    "\n",
    "%matplotlib inline\n",
    "snr = net.get_weight_SNR()\n",
    "print('snr:', len(snr))\n",
    "plt.hist(snr, bins=np.linspace(-0.05, 0.15, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199210\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(net.get_weight_samples(), open('results/BBBweights.pkl','wb'))\n",
    "\n",
    "%matplotlib inline\n",
    "snr = net.get_weight_SNR()\n",
    "\n",
    "print(len(snr))\n",
    "plt.title('Signal to Noise Ratio of Weights')\n",
    "plt.hist(snr, bins=np.linspace(-0.01, 0.35, 250), density=True, alpha=0.6)\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Absolute Value of Signal to Noise Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 0.20M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n",
      "it 0/100, Jtr_pred = 0.454103, err = 0.138933, \u001b[31m   time: 2.770544 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.288004, err = 0.086500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_pred = 0.278006, err = 0.080167, \u001b[31m   time: 2.769514 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.250428, err = 0.074000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_pred = 0.252638, err = 0.073450, \u001b[31m   time: 2.747890 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.254532, err = 0.077400\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_pred = 0.245910, err = 0.071233, \u001b[31m   time: 2.774857 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.237734, err = 0.068400\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_pred = 0.243243, err = 0.070867, \u001b[31m   time: 2.708640 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.246315, err = 0.073500\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_pred = 0.237802, err = 0.067617, \u001b[31m   time: 2.791597 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.231511, err = 0.066900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_pred = 0.240216, err = 0.069100, \u001b[31m   time: 2.676902 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.247934, err = 0.073700\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_pred = 0.239470, err = 0.068833, \u001b[31m   time: 2.681454 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.231630, err = 0.069600\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_pred = 0.236434, err = 0.068967, \u001b[31m   time: 2.853524 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.246994, err = 0.072200\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_pred = 0.237249, err = 0.068700, \u001b[31m   time: 2.652439 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.231966, err = 0.065400\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_pred = 0.234742, err = 0.067850, \u001b[31m   time: 2.716554 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.233491, err = 0.063700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_pred = 0.235296, err = 0.067450, \u001b[31m   time: 2.775318 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.237145, err = 0.070000\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_pred = 0.235185, err = 0.067467, \u001b[31m   time: 2.755247 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.225365, err = 0.064000\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_pred = 0.233700, err = 0.067817, \u001b[31m   time: 2.710486 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.239025, err = 0.069000\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_pred = 0.235328, err = 0.067817, \u001b[31m   time: 2.766700 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.225636, err = 0.064000\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_pred = 0.233300, err = 0.067800, \u001b[31m   time: 2.718061 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.236267, err = 0.069100\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_pred = 0.231088, err = 0.066483, \u001b[31m   time: 2.705116 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.228497, err = 0.067800\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_pred = 0.237012, err = 0.068817, \u001b[31m   time: 2.803765 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.212324, err = 0.060800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_pred = 0.233251, err = 0.067133, \u001b[31m   time: 2.725549 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.209204, err = 0.061000\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_pred = 0.234624, err = 0.067233, \u001b[31m   time: 2.759686 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223973, err = 0.068000\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_pred = 0.232723, err = 0.067317, \u001b[31m   time: 2.739005 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.227975, err = 0.067400\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_pred = 0.235699, err = 0.068350, \u001b[31m   time: 2.696756 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.240846, err = 0.069800\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_pred = 0.235121, err = 0.067500, \u001b[31m   time: 2.645508 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.226932, err = 0.066400\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_pred = 0.232968, err = 0.067467, \u001b[31m   time: 2.732168 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.222906, err = 0.063800\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_pred = 0.234135, err = 0.067517, \u001b[31m   time: 2.664812 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.209945, err = 0.059900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_pred = 0.231695, err = 0.066833, \u001b[31m   time: 2.785376 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.218958, err = 0.067200\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_pred = 0.235514, err = 0.068417, \u001b[31m   time: 2.673388 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.229756, err = 0.064000\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_pred = 0.234422, err = 0.067300, \u001b[31m   time: 2.688609 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223666, err = 0.065600\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_pred = 0.233981, err = 0.067350, \u001b[31m   time: 2.721522 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.221621, err = 0.064000\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_pred = 0.233590, err = 0.067017, \u001b[31m   time: 2.664590 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.236853, err = 0.068300\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_pred = 0.234861, err = 0.068083, \u001b[31m   time: 2.668164 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223913, err = 0.063900\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_pred = 0.231488, err = 0.066150, \u001b[31m   time: 2.739880 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.243271, err = 0.073400\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_pred = 0.231925, err = 0.067317, \u001b[31m   time: 2.694908 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230738, err = 0.064900\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_pred = 0.234357, err = 0.066950, \u001b[31m   time: 2.637855 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230106, err = 0.068900\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_pred = 0.232968, err = 0.067967, \u001b[31m   time: 2.712571 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230463, err = 0.068300\n",
      "\u001b[0m\n",
      "it 35/100, Jtr_pred = 0.236074, err = 0.068250, \u001b[31m   time: 2.688704 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.238929, err = 0.068600\n",
      "\u001b[0m\n",
      "it 36/100, Jtr_pred = 0.232022, err = 0.066867, \u001b[31m   time: 2.768965 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.228660, err = 0.067800\n",
      "\u001b[0m\n",
      "it 37/100, Jtr_pred = 0.232624, err = 0.067383, \u001b[31m   time: 2.708804 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.229666, err = 0.066000\n",
      "\u001b[0m\n",
      "it 38/100, Jtr_pred = 0.232579, err = 0.066750, \u001b[31m   time: 2.661245 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.224627, err = 0.065400\n",
      "\u001b[0m\n",
      "it 39/100, Jtr_pred = 0.233663, err = 0.067117, \u001b[31m   time: 2.737651 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.225969, err = 0.063900\n",
      "\u001b[0m\n",
      "it 40/100, Jtr_pred = 0.234377, err = 0.067817, \u001b[31m   time: 2.774175 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230888, err = 0.067700\n",
      "\u001b[0m\n",
      "it 41/100, Jtr_pred = 0.233616, err = 0.067017, \u001b[31m   time: 2.696604 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.240714, err = 0.072100\n",
      "\u001b[0m\n",
      "it 42/100, Jtr_pred = 0.231953, err = 0.068100, \u001b[31m   time: 2.739044 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.255897, err = 0.077800\n",
      "\u001b[0m\n",
      "it 43/100, Jtr_pred = 0.234084, err = 0.067800, \u001b[31m   time: 2.730370 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.217589, err = 0.062300\n",
      "\u001b[0m\n",
      "it 44/100, Jtr_pred = 0.230015, err = 0.065833, \u001b[31m   time: 2.661667 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.239067, err = 0.069200\n",
      "\u001b[0m\n",
      "it 45/100, Jtr_pred = 0.231908, err = 0.067300, \u001b[31m   time: 2.671032 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.234672, err = 0.065500\n",
      "\u001b[0m\n",
      "it 46/100, Jtr_pred = 0.235570, err = 0.067100, \u001b[31m   time: 2.707085 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.220678, err = 0.064600\n",
      "\u001b[0m\n",
      "it 47/100, Jtr_pred = 0.232410, err = 0.066533, \u001b[31m   time: 2.762866 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.229348, err = 0.065400\n",
      "\u001b[0m\n",
      "it 48/100, Jtr_pred = 0.231374, err = 0.065850, \u001b[31m   time: 2.696454 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.242887, err = 0.071500\n",
      "\u001b[0m\n",
      "it 49/100, Jtr_pred = 0.233697, err = 0.066683, \u001b[31m   time: 2.761539 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.212020, err = 0.063900\n",
      "\u001b[0m\n",
      "it 50/100, Jtr_pred = 0.234130, err = 0.067517, \u001b[31m   time: 2.781114 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.226659, err = 0.066200\n",
      "\u001b[0m\n",
      "it 51/100, Jtr_pred = 0.233127, err = 0.067100, \u001b[31m   time: 2.734543 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.222240, err = 0.064600\n",
      "\u001b[0m\n",
      "it 52/100, Jtr_pred = 0.231997, err = 0.067517, \u001b[31m   time: 2.706896 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230792, err = 0.066900\n",
      "\u001b[0m\n",
      "it 53/100, Jtr_pred = 0.232819, err = 0.066867, \u001b[31m   time: 2.723613 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223182, err = 0.065600\n",
      "\u001b[0m\n",
      "it 54/100, Jtr_pred = 0.232329, err = 0.066650, \u001b[31m   time: 2.687190 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.225108, err = 0.065800\n",
      "\u001b[0m\n",
      "it 55/100, Jtr_pred = 0.233014, err = 0.066367, \u001b[31m   time: 2.687868 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.227881, err = 0.066900\n",
      "\u001b[0m\n",
      "it 56/100, Jtr_pred = 0.234519, err = 0.066633, \u001b[31m   time: 2.745758 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.225209, err = 0.063900\n",
      "\u001b[0m\n",
      "it 57/100, Jtr_pred = 0.232453, err = 0.066567, \u001b[31m   time: 2.702744 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.237170, err = 0.066100\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 58/100, Jtr_pred = 0.234326, err = 0.068183, \u001b[31m   time: 2.755697 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.217709, err = 0.065800\n",
      "\u001b[0m\n",
      "it 59/100, Jtr_pred = 0.233428, err = 0.067533, \u001b[31m   time: 2.792351 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.218949, err = 0.063700\n",
      "\u001b[0m\n",
      "it 60/100, Jtr_pred = 0.233757, err = 0.067350, \u001b[31m   time: 2.691359 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.228788, err = 0.066200\n",
      "\u001b[0m\n",
      "it 61/100, Jtr_pred = 0.233113, err = 0.067617, \u001b[31m   time: 2.753986 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.233171, err = 0.067200\n",
      "\u001b[0m\n",
      "it 62/100, Jtr_pred = 0.233844, err = 0.066933, \u001b[31m   time: 2.729399 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223028, err = 0.063100\n",
      "\u001b[0m\n",
      "it 63/100, Jtr_pred = 0.234571, err = 0.067733, \u001b[31m   time: 2.702128 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.234764, err = 0.068000\n",
      "\u001b[0m\n",
      "it 64/100, Jtr_pred = 0.233877, err = 0.067117, \u001b[31m   time: 2.739097 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.241310, err = 0.071500\n",
      "\u001b[0m\n",
      "it 65/100, Jtr_pred = 0.233636, err = 0.067033, \u001b[31m   time: 2.780540 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.229818, err = 0.067100\n",
      "\u001b[0m\n",
      "it 66/100, Jtr_pred = 0.232772, err = 0.066300, \u001b[31m   time: 2.714498 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.224147, err = 0.065500\n",
      "\u001b[0m\n",
      "it 67/100, Jtr_pred = 0.233666, err = 0.067333, \u001b[31m   time: 2.710328 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.222753, err = 0.063900\n",
      "\u001b[0m\n",
      "it 68/100, Jtr_pred = 0.234707, err = 0.066583, \u001b[31m   time: 2.707094 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.223574, err = 0.068100\n",
      "\u001b[0m\n",
      "it 69/100, Jtr_pred = 0.233054, err = 0.067117, \u001b[31m   time: 2.659167 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.238167, err = 0.070300\n",
      "\u001b[0m\n",
      "it 70/100, Jtr_pred = 0.233351, err = 0.066767, \u001b[31m   time: 2.755995 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.232568, err = 0.068100\n",
      "\u001b[0m\n",
      "it 71/100, Jtr_pred = 0.231387, err = 0.067217, \u001b[31m   time: 2.649634 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.229295, err = 0.066400\n",
      "\u001b[0m\n",
      "it 72/100, Jtr_pred = 0.231153, err = 0.066750, \u001b[31m   time: 2.735950 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.219810, err = 0.063800\n",
      "\u001b[0m\n",
      "it 73/100, Jtr_pred = 0.232450, err = 0.067500, \u001b[31m   time: 2.726489 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.221612, err = 0.062500\n",
      "\u001b[0m\n",
      "it 74/100, Jtr_pred = 0.234549, err = 0.068067, \u001b[31m   time: 2.688783 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.251772, err = 0.076400\n",
      "\u001b[0m\n",
      "it 75/100, Jtr_pred = 0.232712, err = 0.067833, \u001b[31m   time: 2.666774 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.266253, err = 0.080300\n",
      "\u001b[0m\n",
      "it 76/100, Jtr_pred = 0.234406, err = 0.068317, \u001b[31m   time: 2.703645 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.221535, err = 0.064200\n",
      "\u001b[0m\n",
      "it 77/100, Jtr_pred = 0.231799, err = 0.068067, \u001b[31m   time: 2.685375 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.242926, err = 0.069900\n",
      "\u001b[0m\n",
      "it 78/100, Jtr_pred = 0.233058, err = 0.067483, \u001b[31m   time: 2.724795 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.224597, err = 0.067600\n",
      "\u001b[0m\n",
      "it 79/100, Jtr_pred = 0.232266, err = 0.067100, \u001b[31m   time: 2.757272 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.221682, err = 0.063100\n",
      "\u001b[0m\n",
      "it 80/100, Jtr_pred = 0.235175, err = 0.067400, \u001b[31m   time: 2.723640 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.253869, err = 0.074800\n",
      "\u001b[0m\n",
      "it 81/100, Jtr_pred = 0.232399, err = 0.066617, \u001b[31m   time: 2.688226 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.217484, err = 0.063600\n",
      "\u001b[0m\n",
      "it 82/100, Jtr_pred = 0.231295, err = 0.067667, \u001b[31m   time: 2.711605 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.222203, err = 0.065700\n",
      "\u001b[0m\n",
      "it 83/100, Jtr_pred = 0.232014, err = 0.066400, \u001b[31m   time: 2.698620 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.219315, err = 0.065600\n",
      "\u001b[0m\n",
      "it 84/100, Jtr_pred = 0.233580, err = 0.066517, \u001b[31m   time: 2.680946 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.220318, err = 0.064000\n",
      "\u001b[0m\n",
      "it 85/100, Jtr_pred = 0.235602, err = 0.068200, \u001b[31m   time: 2.753495 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.215670, err = 0.061000\n",
      "\u001b[0m\n",
      "it 86/100, Jtr_pred = 0.232286, err = 0.066967, \u001b[31m   time: 2.752702 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.224209, err = 0.066700\n",
      "\u001b[0m\n",
      "it 87/100, Jtr_pred = 0.232053, err = 0.066500, \u001b[31m   time: 2.722735 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.231901, err = 0.066200\n",
      "\u001b[0m\n",
      "it 88/100, Jtr_pred = 0.234390, err = 0.068317, \u001b[31m   time: 2.685337 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.211166, err = 0.059000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 89/100, Jtr_pred = 0.231264, err = 0.067017, \u001b[31m   time: 2.768057 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.221854, err = 0.062400\n",
      "\u001b[0m\n",
      "it 90/100, Jtr_pred = 0.232023, err = 0.066517, \u001b[31m   time: 2.697488 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.212805, err = 0.059400\n",
      "\u001b[0m\n",
      "it 91/100, Jtr_pred = 0.234297, err = 0.067450, \u001b[31m   time: 2.724757 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.236562, err = 0.071000\n",
      "\u001b[0m\n",
      "it 92/100, Jtr_pred = 0.232882, err = 0.068217, \u001b[31m   time: 2.729497 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.215930, err = 0.059600\n",
      "\u001b[0m\n",
      "it 93/100, Jtr_pred = 0.232580, err = 0.067850, \u001b[31m   time: 2.771446 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230446, err = 0.064700\n",
      "\u001b[0m\n",
      "it 94/100, Jtr_pred = 0.232912, err = 0.067117, \u001b[31m   time: 2.648799 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.243388, err = 0.070100\n",
      "\u001b[0m\n",
      "it 95/100, Jtr_pred = 0.229489, err = 0.066317, \u001b[31m   time: 2.708694 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.235144, err = 0.071500\n",
      "\u001b[0m\n",
      "it 96/100, Jtr_pred = 0.231925, err = 0.066933, \u001b[31m   time: 2.730789 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.231444, err = 0.066600\n",
      "\u001b[0m\n",
      "it 97/100, Jtr_pred = 0.231084, err = 0.066167, \u001b[31m   time: 2.662261 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.227921, err = 0.067700\n",
      "\u001b[0m\n",
      "it 98/100, Jtr_pred = 0.230425, err = 0.066900, \u001b[31m   time: 2.666611 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.224492, err = 0.063700\n",
      "\u001b[0m\n",
      "it 99/100, Jtr_pred = 0.230349, err = 0.067100, \u001b[31m   time: 2.744553 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.241358, err = 0.072300\n",
      "\u001b[0m\n",
      "\u001b[31m   average time: 3.239397 seconds\n",
      "\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_last.dat\n",
      "\u001b[0m\n",
      "\u001b[36m\n",
      "RESULTS:\u001b[0m\n",
      "  cost_dev: 0.209204 (cost_train 0.229489)\n",
      "  err_dev: 0.059000\n",
      "  nb_parameters: 199210 (194.54KB)\n",
      "  time_per_it: 3.239397s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "from MC_dropout.model import *\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "models_dir = 'dropout'\n",
    "results_dir = 'dropout'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "NTrainPointsMNIST = 60000\n",
    "batch_size = 128\n",
    "nb_epochs = 100\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-3\n",
    "########################################################################################\n",
    "\n",
    "net = MC_drop_net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n",
    "                  weight_decay=1, n_hid=200)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "epoch = 0\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost_pred, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "net.save(models_dir+'/theta_last.dat')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "## Save results for plots\n",
    "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_weights = net.get_weight_samples()\n",
    "pickle.dump(dropout_weights, open('results/dropoutweights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "#         self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "nhid = 200\n",
    "net = Net(784, nhid, 10)\n",
    "net.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/468], Loss: 0.3100\n",
      "Epoch [1/100], Step [200/468], Loss: 0.1243\n",
      "Epoch [1/100], Step [300/468], Loss: 0.1346\n",
      "Epoch [1/100], Step [400/468], Loss: 0.0481\n",
      "Epoch [2/100], Step [100/468], Loss: 0.0790\n",
      "Epoch [2/100], Step [200/468], Loss: 0.0758\n",
      "Epoch [2/100], Step [300/468], Loss: 0.1125\n",
      "Epoch [2/100], Step [400/468], Loss: 0.1132\n",
      "Epoch [3/100], Step [100/468], Loss: 0.0902\n",
      "Epoch [3/100], Step [200/468], Loss: 0.0447\n",
      "Epoch [3/100], Step [300/468], Loss: 0.0722\n",
      "Epoch [3/100], Step [400/468], Loss: 0.1120\n",
      "Epoch [4/100], Step [100/468], Loss: 0.1222\n",
      "Epoch [4/100], Step [200/468], Loss: 0.0495\n",
      "Epoch [4/100], Step [300/468], Loss: 0.1077\n",
      "Epoch [4/100], Step [400/468], Loss: 0.0124\n",
      "Epoch [5/100], Step [100/468], Loss: 0.0151\n",
      "Epoch [5/100], Step [200/468], Loss: 0.0312\n",
      "Epoch [5/100], Step [300/468], Loss: 0.0164\n",
      "Epoch [5/100], Step [400/468], Loss: 0.0188\n",
      "Epoch [6/100], Step [100/468], Loss: 0.0165\n",
      "Epoch [6/100], Step [200/468], Loss: 0.0222\n",
      "Epoch [6/100], Step [300/468], Loss: 0.0029\n",
      "Epoch [6/100], Step [400/468], Loss: 0.0084\n",
      "Epoch [7/100], Step [100/468], Loss: 0.0180\n",
      "Epoch [7/100], Step [200/468], Loss: 0.0058\n",
      "Epoch [7/100], Step [300/468], Loss: 0.0087\n",
      "Epoch [7/100], Step [400/468], Loss: 0.0258\n",
      "Epoch [8/100], Step [100/468], Loss: 0.0048\n",
      "Epoch [8/100], Step [200/468], Loss: 0.0155\n",
      "Epoch [8/100], Step [300/468], Loss: 0.0648\n",
      "Epoch [8/100], Step [400/468], Loss: 0.0085\n",
      "Epoch [9/100], Step [100/468], Loss: 0.0025\n",
      "Epoch [9/100], Step [200/468], Loss: 0.0117\n",
      "Epoch [9/100], Step [300/468], Loss: 0.0037\n",
      "Epoch [9/100], Step [400/468], Loss: 0.0346\n",
      "Epoch [10/100], Step [100/468], Loss: 0.0033\n",
      "Epoch [10/100], Step [200/468], Loss: 0.0519\n",
      "Epoch [10/100], Step [300/468], Loss: 0.0075\n",
      "Epoch [10/100], Step [400/468], Loss: 0.0137\n",
      "Epoch [11/100], Step [100/468], Loss: 0.0067\n",
      "Epoch [11/100], Step [200/468], Loss: 0.0288\n",
      "Epoch [11/100], Step [300/468], Loss: 0.0013\n",
      "Epoch [11/100], Step [400/468], Loss: 0.0017\n",
      "Epoch [12/100], Step [100/468], Loss: 0.0064\n",
      "Epoch [12/100], Step [200/468], Loss: 0.0164\n",
      "Epoch [12/100], Step [300/468], Loss: 0.0010\n",
      "Epoch [12/100], Step [400/468], Loss: 0.0740\n",
      "Epoch [13/100], Step [100/468], Loss: 0.0092\n",
      "Epoch [13/100], Step [200/468], Loss: 0.0034\n",
      "Epoch [13/100], Step [300/468], Loss: 0.0189\n",
      "Epoch [13/100], Step [400/468], Loss: 0.0208\n",
      "Epoch [14/100], Step [100/468], Loss: 0.0025\n",
      "Epoch [14/100], Step [200/468], Loss: 0.0225\n",
      "Epoch [14/100], Step [300/468], Loss: 0.0016\n",
      "Epoch [14/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [15/100], Step [100/468], Loss: 0.0022\n",
      "Epoch [15/100], Step [200/468], Loss: 0.0060\n",
      "Epoch [15/100], Step [300/468], Loss: 0.0099\n",
      "Epoch [15/100], Step [400/468], Loss: 0.0025\n",
      "Epoch [16/100], Step [100/468], Loss: 0.0007\n",
      "Epoch [16/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [16/100], Step [300/468], Loss: 0.0110\n",
      "Epoch [16/100], Step [400/468], Loss: 0.0017\n",
      "Epoch [17/100], Step [100/468], Loss: 0.0018\n",
      "Epoch [17/100], Step [200/468], Loss: 0.0144\n",
      "Epoch [17/100], Step [300/468], Loss: 0.0414\n",
      "Epoch [17/100], Step [400/468], Loss: 0.0006\n",
      "Epoch [18/100], Step [100/468], Loss: 0.0042\n",
      "Epoch [18/100], Step [200/468], Loss: 0.0126\n",
      "Epoch [18/100], Step [300/468], Loss: 0.0017\n",
      "Epoch [18/100], Step [400/468], Loss: 0.0920\n",
      "Epoch [19/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [19/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [19/100], Step [300/468], Loss: 0.0008\n",
      "Epoch [19/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [20/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [20/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [20/100], Step [300/468], Loss: 0.0007\n",
      "Epoch [20/100], Step [400/468], Loss: 0.0148\n",
      "Epoch [21/100], Step [100/468], Loss: 0.0007\n",
      "Epoch [21/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [21/100], Step [300/468], Loss: 0.0058\n",
      "Epoch [21/100], Step [400/468], Loss: 0.0171\n",
      "Epoch [22/100], Step [100/468], Loss: 0.0366\n",
      "Epoch [22/100], Step [200/468], Loss: 0.0007\n",
      "Epoch [22/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [22/100], Step [400/468], Loss: 0.0015\n",
      "Epoch [23/100], Step [100/468], Loss: 0.0039\n",
      "Epoch [23/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [23/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [23/100], Step [400/468], Loss: 0.0024\n",
      "Epoch [24/100], Step [100/468], Loss: 0.0085\n",
      "Epoch [24/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [24/100], Step [300/468], Loss: 0.0334\n",
      "Epoch [24/100], Step [400/468], Loss: 0.0068\n",
      "Epoch [25/100], Step [100/468], Loss: 0.0031\n",
      "Epoch [25/100], Step [200/468], Loss: 0.0006\n",
      "Epoch [25/100], Step [300/468], Loss: 0.0076\n",
      "Epoch [25/100], Step [400/468], Loss: 0.0081\n",
      "Epoch [26/100], Step [100/468], Loss: 0.0345\n",
      "Epoch [26/100], Step [200/468], Loss: 0.0028\n",
      "Epoch [26/100], Step [300/468], Loss: 0.0007\n",
      "Epoch [26/100], Step [400/468], Loss: 0.0012\n",
      "Epoch [27/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [27/100], Step [200/468], Loss: 0.0006\n",
      "Epoch [27/100], Step [300/468], Loss: 0.0138\n",
      "Epoch [27/100], Step [400/468], Loss: 0.0030\n",
      "Epoch [28/100], Step [100/468], Loss: 0.0004\n",
      "Epoch [28/100], Step [200/468], Loss: 0.0471\n",
      "Epoch [28/100], Step [300/468], Loss: 0.0235\n",
      "Epoch [28/100], Step [400/468], Loss: 0.0053\n",
      "Epoch [29/100], Step [100/468], Loss: 0.0121\n",
      "Epoch [29/100], Step [200/468], Loss: 0.0137\n",
      "Epoch [29/100], Step [300/468], Loss: 0.0059\n",
      "Epoch [29/100], Step [400/468], Loss: 0.0011\n",
      "Epoch [30/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [30/100], Step [200/468], Loss: 0.0016\n",
      "Epoch [30/100], Step [300/468], Loss: 0.0271\n",
      "Epoch [30/100], Step [400/468], Loss: 0.0010\n",
      "Epoch [31/100], Step [100/468], Loss: 0.0491\n",
      "Epoch [31/100], Step [200/468], Loss: 0.0522\n",
      "Epoch [31/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [31/100], Step [400/468], Loss: 0.0043\n",
      "Epoch [32/100], Step [100/468], Loss: 0.0395\n",
      "Epoch [32/100], Step [200/468], Loss: 0.0025\n",
      "Epoch [32/100], Step [300/468], Loss: 0.0057\n",
      "Epoch [32/100], Step [400/468], Loss: 0.0278\n",
      "Epoch [33/100], Step [100/468], Loss: 0.0057\n",
      "Epoch [33/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [33/100], Step [300/468], Loss: 0.0004\n",
      "Epoch [33/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [34/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [34/100], Step [200/468], Loss: 0.0188\n",
      "Epoch [34/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [34/100], Step [400/468], Loss: 0.0030\n",
      "Epoch [35/100], Step [100/468], Loss: 0.0473\n",
      "Epoch [35/100], Step [200/468], Loss: 0.0334\n",
      "Epoch [35/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [35/100], Step [400/468], Loss: 0.0457\n",
      "Epoch [36/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [36/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [36/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [36/100], Step [400/468], Loss: 0.0013\n",
      "Epoch [37/100], Step [100/468], Loss: 0.0021\n",
      "Epoch [37/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [37/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [37/100], Step [400/468], Loss: 0.0006\n",
      "Epoch [38/100], Step [100/468], Loss: 0.0006\n",
      "Epoch [38/100], Step [200/468], Loss: 0.0002\n",
      "Epoch [38/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [38/100], Step [400/468], Loss: 0.0020\n",
      "Epoch [39/100], Step [100/468], Loss: 0.0016\n",
      "Epoch [39/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [39/100], Step [300/468], Loss: 0.0082\n",
      "Epoch [39/100], Step [400/468], Loss: 0.0026\n",
      "Epoch [40/100], Step [100/468], Loss: 0.0004\n",
      "Epoch [40/100], Step [200/468], Loss: 0.0095\n",
      "Epoch [40/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [40/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [41/100], Step [100/468], Loss: 0.0036\n",
      "Epoch [41/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [41/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [41/100], Step [400/468], Loss: 0.0105\n",
      "Epoch [42/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [42/100], Step [200/468], Loss: 0.0010\n",
      "Epoch [42/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [42/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [43/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [43/100], Step [200/468], Loss: 0.0465\n",
      "Epoch [43/100], Step [300/468], Loss: 0.0043\n",
      "Epoch [43/100], Step [400/468], Loss: 0.0570\n",
      "Epoch [44/100], Step [100/468], Loss: 0.0231\n",
      "Epoch [44/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [44/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [44/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [45/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [45/100], Step [200/468], Loss: 0.0006\n",
      "Epoch [45/100], Step [300/468], Loss: 0.0013\n",
      "Epoch [45/100], Step [400/468], Loss: 0.0008\n",
      "Epoch [46/100], Step [100/468], Loss: 0.0009\n",
      "Epoch [46/100], Step [200/468], Loss: 0.0159\n",
      "Epoch [46/100], Step [300/468], Loss: 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Step [400/468], Loss: 0.0013\n",
      "Epoch [47/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [47/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [47/100], Step [300/468], Loss: 0.0026\n",
      "Epoch [47/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [48/100], Step [100/468], Loss: 0.0003\n",
      "Epoch [48/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [48/100], Step [300/468], Loss: 0.0404\n",
      "Epoch [48/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [49/100], Step [100/468], Loss: 0.0004\n",
      "Epoch [49/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [49/100], Step [300/468], Loss: 0.0046\n",
      "Epoch [49/100], Step [400/468], Loss: 0.0010\n",
      "Epoch [50/100], Step [100/468], Loss: 0.0008\n",
      "Epoch [50/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [50/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [100/468], Loss: 0.0058\n",
      "Epoch [51/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [52/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [52/100], Step [200/468], Loss: 0.0012\n",
      "Epoch [52/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [52/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [53/100], Step [100/468], Loss: 0.0024\n",
      "Epoch [53/100], Step [200/468], Loss: 0.0009\n",
      "Epoch [53/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [53/100], Step [400/468], Loss: 0.0075\n",
      "Epoch [54/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [54/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [54/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [54/100], Step [400/468], Loss: 0.0065\n",
      "Epoch [55/100], Step [100/468], Loss: 0.0020\n",
      "Epoch [55/100], Step [200/468], Loss: 0.0013\n",
      "Epoch [55/100], Step [300/468], Loss: 0.2888\n",
      "Epoch [55/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [56/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [56/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [56/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [56/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [57/100], Step [100/468], Loss: 0.0012\n",
      "Epoch [57/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [57/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [57/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [58/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [58/100], Step [200/468], Loss: 0.0364\n",
      "Epoch [58/100], Step [300/468], Loss: 0.0248\n",
      "Epoch [58/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [59/100], Step [100/468], Loss: 0.0035\n",
      "Epoch [59/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [59/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [59/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [60/100], Step [100/468], Loss: 0.0004\n",
      "Epoch [60/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [60/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [60/100], Step [400/468], Loss: 0.0028\n",
      "Epoch [61/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [61/100], Step [200/468], Loss: 0.0017\n",
      "Epoch [61/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [61/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [62/100], Step [100/468], Loss: 0.0208\n",
      "Epoch [62/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [62/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [62/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [63/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [63/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [63/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [63/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [64/100], Step [100/468], Loss: 0.0026\n",
      "Epoch [64/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [64/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [64/100], Step [400/468], Loss: 0.0405\n",
      "Epoch [65/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [65/100], Step [200/468], Loss: 0.0042\n",
      "Epoch [65/100], Step [300/468], Loss: 0.0052\n",
      "Epoch [65/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [66/100], Step [100/468], Loss: 0.0126\n",
      "Epoch [66/100], Step [200/468], Loss: 0.0002\n",
      "Epoch [66/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [66/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [67/100], Step [100/468], Loss: 0.0005\n",
      "Epoch [67/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [67/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [67/100], Step [400/468], Loss: 0.0010\n",
      "Epoch [68/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [200/468], Loss: 0.0114\n",
      "Epoch [68/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [100/468], Loss: 0.0755\n",
      "Epoch [69/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [69/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [400/468], Loss: 0.0007\n",
      "Epoch [72/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [73/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [73/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [73/100], Step [300/468], Loss: 0.0995\n",
      "Epoch [73/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [74/100], Step [100/468], Loss: 0.0153\n",
      "Epoch [74/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [74/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [74/100], Step [400/468], Loss: 0.0046\n",
      "Epoch [75/100], Step [100/468], Loss: 0.0050\n",
      "Epoch [75/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [75/100], Step [300/468], Loss: 0.0349\n",
      "Epoch [75/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [200/468], Loss: 0.0011\n",
      "Epoch [76/100], Step [300/468], Loss: 0.0005\n",
      "Epoch [76/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [100/468], Loss: 0.0003\n",
      "Epoch [77/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [300/468], Loss: 0.0026\n",
      "Epoch [78/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [79/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [300/468], Loss: 0.0036\n",
      "Epoch [79/100], Step [400/468], Loss: 0.0160\n",
      "Epoch [80/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [80/100], Step [200/468], Loss: 0.0009\n",
      "Epoch [80/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [80/100], Step [400/468], Loss: 0.0600\n",
      "Epoch [81/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [81/100], Step [200/468], Loss: 0.0053\n",
      "Epoch [81/100], Step [300/468], Loss: 0.0061\n",
      "Epoch [81/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [82/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [83/100], Step [100/468], Loss: 0.0055\n",
      "Epoch [83/100], Step [200/468], Loss: 0.0015\n",
      "Epoch [83/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [83/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [85/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [400/468], Loss: 0.0048\n",
      "Epoch [86/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [86/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [86/100], Step [300/468], Loss: 0.0007\n",
      "Epoch [86/100], Step [400/468], Loss: 0.1030\n",
      "Epoch [87/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [87/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [87/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [87/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [88/100], Step [100/468], Loss: 0.0045\n",
      "Epoch [88/100], Step [200/468], Loss: 0.0010\n",
      "Epoch [88/100], Step [300/468], Loss: 0.0249\n",
      "Epoch [88/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [89/100], Step [100/468], Loss: 0.0005\n",
      "Epoch [89/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [89/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [89/100], Step [400/468], Loss: 0.0032\n",
      "Epoch [90/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [300/468], Loss: 0.0044\n",
      "Epoch [90/100], Step [400/468], Loss: 0.0010\n",
      "Epoch [91/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [91/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [91/100], Step [300/468], Loss: 0.0023\n",
      "Epoch [91/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [92/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [92/100], Step [200/468], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [92/100], Step [400/468], Loss: 0.0133\n",
      "Epoch [93/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [93/100], Step [200/468], Loss: 0.0125\n",
      "Epoch [93/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [93/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [94/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [95/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [200/468], Loss: 0.0026\n",
      "Epoch [95/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [400/468], Loss: 0.1807\n",
      "Epoch [96/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [96/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [96/100], Step [300/468], Loss: 0.0020\n",
      "Epoch [96/100], Step [400/468], Loss: 0.0023\n",
      "Epoch [97/100], Step [100/468], Loss: 0.0385\n",
      "Epoch [97/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [97/100], Step [300/468], Loss: 0.0008\n",
      "Epoch [97/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [300/468], Loss: 0.0060\n",
      "Epoch [99/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [400/468], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size=128\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):   # Load a batch of images with its (index, data, class)\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images.cuda())                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels.cuda())                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0: # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, \n",
    "                   num_epochs, \n",
    "                   i+1, \n",
    "                   len(trainset)//batch_size, \n",
    "                   loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images:  98.25\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in valloader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images.cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels.cuda()).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on test images: ', 100 * correct.item() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.concatenate((net.fc1.weight.cpu().data.reshape(nhid*784), net.fc2.weight.cpu().data.reshape(nhid**2), net.fc3.weight.cpu().data.reshape(nhid*10)))\n",
    "pickle.dump(weights, open('results/ffnnweights.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988000\n",
      "198800\n",
      "198800\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "bbb_weights = pickle.load(open('results/BBBweights.pkl', 'rb'))\n",
    "dropout_weights = pickle.load(open('results/dropoutweights.pkl', 'rb'))\n",
    "ffnn_weights = pickle.load(open('results/ffnnweights.pkl', 'rb'))\n",
    "\n",
    "# pickle.dump(bbb_weights, open('BBBweights256.pkl', 'rb'))\n",
    "# pickle.dump(dropout_weights, open('dropoutweights256.pkl', 'rb'))\n",
    "# pickle.dump(ffnn_weights, open('SGDweights256.pkl', 'rb'))\n",
    "\n",
    "\n",
    "print(len(bbb_weights))\n",
    "print(len(dropout_weights))\n",
    "print(len(ffnn_weights))\n",
    "\n",
    "# dropout_weights = net.get_weight_samples()\n",
    "\n",
    "plt.hist(bbb_weights, density=True, bins=np.linspace(-0.1, 0.1, 50), alpha=0.5, label='Bayes by Backprop')\n",
    "plt.hist(dropout_weights, density=True, bins=np.linspace(-0.1, 0.1, 50), alpha=0.5, label='Vanilla SGD')\n",
    "plt.hist(ffnn_weights, density=True, bins=np.linspace(-0.1, 0.1, 50), alpha=0.5, label='MC Dropout')\n",
    "plt.legend()\n",
    "plt.xlim((-0.1, 0.1))\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Sampled Weights')\n",
    "plt.title('Distribution of Weights')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(bbb_weights, open('BBBweights1200.pkl', 'wb'))\n",
    "# pickle.dump(dropout_weights, open('dropoutweights1200.pkl', 'wb'))\n",
    "# pickle.dump(ffnn_weights, open('SGDweights1200.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.5, 0.75, 0.95, 0.98]\n",
      "-inf\n",
      "params: 199210\n",
      "Delete proportion: 0.000000\n",
      "\u001b[34m    Jtest = 0.107873, err = 0.025000\n",
      "\u001b[0m\n",
      "-10.731100 dB\n",
      "params: 99605\n",
      "Delete proportion: 0.500000\n",
      "\u001b[34m    Jtest = 0.099280, err = 0.024700\n",
      "\u001b[0m\n",
      "-8.541501 dB\n",
      "params: 49803\n",
      "Delete proportion: 0.750000\n",
      "\u001b[34m    Jtest = 0.096376, err = 0.024700\n",
      "\u001b[0m\n",
      "-2.043322 dB\n",
      "params: 9961\n",
      "Delete proportion: 0.950000\n",
      "\u001b[34m    Jtest = 0.103565, err = 0.028500\n",
      "\u001b[0m\n",
      "0.739247 dB\n",
      "params: 3985\n",
      "Delete proportion: 0.980000\n",
      "\u001b[34m    Jtest = 0.188387, err = 0.050000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f04088cfc50>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create pruning graphs\n",
    "net = torch.load('bbb.net')\n",
    "SNR_vector = net.get_weight_SNR()\n",
    "\n",
    "delete_proportions = [0, 0.5, 0.75, 0.95, 0.98]\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "# keep_proportions = 1 - delete_proportions\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "Nsamples_predict = 100\n",
    "# Nsamples_KLD = 20\n",
    "use_cuda = torch.cuda.is_available()\n",
    "net.set_mode_train(False)\n",
    "\n",
    "if use_cuda:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "else:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=4)\n",
    "err_vec = np.zeros(len(delete_proportions))\n",
    "\n",
    "print(delete_proportions)\n",
    "\n",
    "for idx, p in enumerate(delete_proportions):\n",
    "    if p > 0:\n",
    "        min_snr = np.percentile(SNR_vector, p*100)\n",
    "        print('%f dB' % (10*np.log10(min_snr)))\n",
    "        og_state_dict, n_unmask = net.mask_model(Nsamples=0, thresh=min_snr)\n",
    "#         print(net.model.state_dict()) # for debug purposes\n",
    "        print('params: %d' % (n_unmask))\n",
    "    else:\n",
    "        print('-inf')\n",
    "        print('params: %d' % (net.get_nb_parameters()/2))\n",
    "    \n",
    "    test_cost = 0  # Note that these are per sample\n",
    "    test_err = 0\n",
    "    nb_samples = 0\n",
    "    for j, (x, y) in enumerate(valloader):\n",
    "        cost, err, probs = net.sample_eval(x, y, Nsamples_predict, logits=False) # , logits=True\n",
    "\n",
    "        test_cost += cost\n",
    "        test_err += err.cpu().numpy()\n",
    "#         test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    test_cost /= nb_samples\n",
    "    test_err /= nb_samples\n",
    "\n",
    "    print('Delete proportion: %f' % (p))\n",
    "    cprint('b', '    Jtest = %1.6f, err = %1.6f\\n' % (test_cost, test_err))\n",
    "    err_vec[idx] = test_err\n",
    "    \n",
    "    if p > 0:\n",
    "        net.model.load_state_dict(og_state_dict)\n",
    "    \n",
    "    \n",
    "plt.figure(dpi=100)\n",
    "plt.plot(delete_proportions, err_vec, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
