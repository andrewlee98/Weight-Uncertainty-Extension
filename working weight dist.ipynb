{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "from Bayes_By_Backprop.model import *\n",
    "from Bayes_By_Backprop_Local_Reparametrization.model import *\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 4.79M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n",
      "it 0/100, Jtr_KL = 10.166409, Jtr_pred = 1.926458, err = 0.361650, \u001b[31m   time: 22.861059 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.424566, err = 0.120000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_KL = 9.105092, Jtr_pred = 0.563709, err = 0.159117, \u001b[31m   time: 7.928703 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.324547, err = 0.089900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_KL = 8.213935, Jtr_pred = 0.468553, err = 0.130550, \u001b[31m   time: 8.146134 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.273314, err = 0.073400\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_KL = 7.478907, Jtr_pred = 0.414718, err = 0.116067, \u001b[31m   time: 8.166052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.254682, err = 0.071900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_KL = 6.875761, Jtr_pred = 0.376852, err = 0.107750, \u001b[31m   time: 8.282214 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.230029, err = 0.068200\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_KL = 6.378636, Jtr_pred = 0.351806, err = 0.097817, \u001b[31m   time: 8.383389 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.204158, err = 0.059700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_KL = 5.969666, Jtr_pred = 0.333672, err = 0.092167, \u001b[31m   time: 8.264819 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.184035, err = 0.055600\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_KL = 5.633685, Jtr_pred = 0.318022, err = 0.090200, \u001b[31m   time: 8.053383 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.180466, err = 0.053500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_KL = 5.356868, Jtr_pred = 0.302799, err = 0.084683, \u001b[31m   time: 8.067984 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.185343, err = 0.054800\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_KL = 5.129031, Jtr_pred = 0.296113, err = 0.082500, \u001b[31m   time: 8.035430 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.158960, err = 0.048700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_KL = 4.940291, Jtr_pred = 0.282934, err = 0.079117, \u001b[31m   time: 7.963581 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.148802, err = 0.045200\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_KL = 4.783854, Jtr_pred = 0.274621, err = 0.076717, \u001b[31m   time: 8.981563 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142936, err = 0.043500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_KL = 4.655408, Jtr_pred = 0.269493, err = 0.075817, \u001b[31m   time: 7.884417 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.141300, err = 0.041100\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_KL = 4.548205, Jtr_pred = 0.261611, err = 0.073650, \u001b[31m   time: 8.361543 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.137145, err = 0.041000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_KL = 4.458697, Jtr_pred = 0.257109, err = 0.072633, \u001b[31m   time: 8.521095 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.130570, err = 0.037500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_KL = 4.383448, Jtr_pred = 0.249568, err = 0.070133, \u001b[31m   time: 8.167970 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.122449, err = 0.037500\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_KL = 4.320648, Jtr_pred = 0.249475, err = 0.069883, \u001b[31m   time: 7.918215 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.120598, err = 0.035900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_KL = 4.268073, Jtr_pred = 0.244385, err = 0.068133, \u001b[31m   time: 8.076021 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.126008, err = 0.039700\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_KL = 4.222898, Jtr_pred = 0.240514, err = 0.067383, \u001b[31m   time: 8.008137 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.129578, err = 0.040600\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_KL = 4.184560, Jtr_pred = 0.234561, err = 0.067450, \u001b[31m   time: 8.362093 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.108410, err = 0.032800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_KL = 4.151779, Jtr_pred = 0.230567, err = 0.065333, \u001b[31m   time: 8.029883 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.116605, err = 0.034600\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_KL = 4.122110, Jtr_pred = 0.227753, err = 0.064600, \u001b[31m   time: 7.792152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.111581, err = 0.033000\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_KL = 4.097665, Jtr_pred = 0.228675, err = 0.064833, \u001b[31m   time: 8.347614 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.105172, err = 0.031000\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_KL = 4.075780, Jtr_pred = 0.224608, err = 0.064100, \u001b[31m   time: 7.900297 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.109550, err = 0.032800\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_KL = 4.056279, Jtr_pred = 0.224024, err = 0.064033, \u001b[31m   time: 8.055044 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.107982, err = 0.032700\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_KL = 4.038311, Jtr_pred = 0.217448, err = 0.061033, \u001b[31m   time: 7.709958 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.106246, err = 0.033000\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_KL = 4.022607, Jtr_pred = 0.220350, err = 0.063700, \u001b[31m   time: 8.396473 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.103756, err = 0.032400\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_KL = 4.008338, Jtr_pred = 0.215512, err = 0.062400, \u001b[31m   time: 7.772248 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.103996, err = 0.029900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_KL = 3.994262, Jtr_pred = 0.212076, err = 0.060417, \u001b[31m   time: 8.187023 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.104971, err = 0.032200\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_KL = 3.981396, Jtr_pred = 0.209210, err = 0.060350, \u001b[31m   time: 7.876704 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.098928, err = 0.031000\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_KL = 3.968717, Jtr_pred = 0.207117, err = 0.059517, \u001b[31m   time: 8.230567 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.097006, err = 0.028500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_KL = 3.957325, Jtr_pred = 0.210057, err = 0.059950, \u001b[31m   time: 8.207124 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.098303, err = 0.029600\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_KL = 3.946763, Jtr_pred = 0.203673, err = 0.057917, \u001b[31m   time: 7.949501 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.099345, err = 0.030200\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_KL = 3.936253, Jtr_pred = 0.205074, err = 0.059550, \u001b[31m   time: 8.031343 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.103611, err = 0.033500\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_KL = 3.926674, Jtr_pred = 0.203667, err = 0.058267, \u001b[31m   time: 7.967429 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.096751, err = 0.029100\n",
      "\u001b[0m\n",
      "it 35/100, Jtr_KL = 3.917208, Jtr_pred = 0.201466, err = 0.057783, \u001b[31m   time: 7.731809 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.103383, err = 0.030500\n",
      "\u001b[0m\n",
      "it 36/100, Jtr_KL = 3.907853, Jtr_pred = 0.200469, err = 0.058200, \u001b[31m   time: 8.267042 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.095575, err = 0.027700\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 37/100, Jtr_KL = 3.898438, Jtr_pred = 0.200374, err = 0.057267, \u001b[31m   time: 8.223724 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.103426, err = 0.031500\n",
      "\u001b[0m\n",
      "it 38/100, Jtr_KL = 3.889338, Jtr_pred = 0.198467, err = 0.057450, \u001b[31m   time: 8.392478 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.104289, err = 0.030700\n",
      "\u001b[0m\n",
      "it 39/100, Jtr_KL = 3.880122, Jtr_pred = 0.197956, err = 0.057117, \u001b[31m   time: 8.099424 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.101652, err = 0.031700\n",
      "\u001b[0m\n",
      "it 40/100, Jtr_KL = 3.871293, Jtr_pred = 0.200389, err = 0.058067, \u001b[31m   time: 7.933527 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.099273, err = 0.031200\n",
      "\u001b[0m\n",
      "it 41/100, Jtr_KL = 3.862969, Jtr_pred = 0.195681, err = 0.057433, \u001b[31m   time: 8.034750 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.097629, err = 0.031600\n",
      "\u001b[0m\n",
      "it 42/100, Jtr_KL = 3.854310, Jtr_pred = 0.194455, err = 0.056283, \u001b[31m   time: 7.981107 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093859, err = 0.030600\n",
      "\u001b[0m\n",
      "it 43/100, Jtr_KL = 3.845821, Jtr_pred = 0.197111, err = 0.057633, \u001b[31m   time: 8.132963 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.106668, err = 0.034900\n",
      "\u001b[0m\n",
      "it 44/100, Jtr_KL = 3.837654, Jtr_pred = 0.193455, err = 0.057300, \u001b[31m   time: 8.351877 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091747, err = 0.028400\n",
      "\u001b[0m\n",
      "it 45/100, Jtr_KL = 3.829460, Jtr_pred = 0.192494, err = 0.056583, \u001b[31m   time: 8.042393 seconds\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m    Jdev = 0.097237, err = 0.030500\n",
      "\u001b[0m\n",
      "it 46/100, Jtr_KL = 3.821155, Jtr_pred = 0.192635, err = 0.055733, \u001b[31m   time: 8.425806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088831, err = 0.027100\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 47/100, Jtr_KL = 3.813319, Jtr_pred = 0.193638, err = 0.056867, \u001b[31m   time: 8.170145 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.096276, err = 0.031100\n",
      "\u001b[0m\n",
      "it 48/100, Jtr_KL = 3.805074, Jtr_pred = 0.191524, err = 0.056350, \u001b[31m   time: 8.126693 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089048, err = 0.027600\n",
      "\u001b[0m\n",
      "it 49/100, Jtr_KL = 3.796826, Jtr_pred = 0.187266, err = 0.054450, \u001b[31m   time: 7.907090 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.102581, err = 0.031700\n",
      "\u001b[0m\n",
      "it 50/100, Jtr_KL = 3.788303, Jtr_pred = 0.189231, err = 0.055717, \u001b[31m   time: 8.306877 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.094987, err = 0.029700\n",
      "\u001b[0m\n",
      "it 51/100, Jtr_KL = 3.780498, Jtr_pred = 0.187284, err = 0.054617, \u001b[31m   time: 7.800674 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.105023, err = 0.033700\n",
      "\u001b[0m\n",
      "it 52/100, Jtr_KL = 3.772824, Jtr_pred = 0.187630, err = 0.054917, \u001b[31m   time: 8.560981 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090156, err = 0.028100\n",
      "\u001b[0m\n",
      "it 53/100, Jtr_KL = 3.764710, Jtr_pred = 0.187433, err = 0.055017, \u001b[31m   time: 7.781842 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090881, err = 0.027300\n",
      "\u001b[0m\n",
      "it 54/100, Jtr_KL = 3.756796, Jtr_pred = 0.185071, err = 0.053433, \u001b[31m   time: 8.005442 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091034, err = 0.028800\n",
      "\u001b[0m\n",
      "it 55/100, Jtr_KL = 3.748883, Jtr_pred = 0.186734, err = 0.054583, \u001b[31m   time: 8.049401 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.085199, err = 0.025500\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_best.dat\n",
      "\u001b[0m\n",
      "it 56/100, Jtr_KL = 3.740654, Jtr_pred = 0.184362, err = 0.053450, \u001b[31m   time: 7.901511 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091216, err = 0.029100\n",
      "\u001b[0m\n",
      "it 57/100, Jtr_KL = 3.732699, Jtr_pred = 0.185721, err = 0.055333, \u001b[31m   time: 8.446308 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086664, err = 0.026400\n",
      "\u001b[0m\n",
      "it 58/100, Jtr_KL = 3.724984, Jtr_pred = 0.184602, err = 0.054117, \u001b[31m   time: 8.064090 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.101125, err = 0.032600\n",
      "\u001b[0m\n",
      "it 59/100, Jtr_KL = 3.717273, Jtr_pred = 0.184720, err = 0.054383, \u001b[31m   time: 8.739861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.087244, err = 0.026100\n",
      "\u001b[0m\n",
      "it 60/100, Jtr_KL = 3.709693, Jtr_pred = 0.184104, err = 0.052933, \u001b[31m   time: 8.079033 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.094582, err = 0.029000\n",
      "\u001b[0m\n",
      "it 61/100, Jtr_KL = 3.701883, Jtr_pred = 0.183640, err = 0.053550, \u001b[31m   time: 7.993404 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090293, err = 0.029100\n",
      "\u001b[0m\n",
      "it 62/100, Jtr_KL = 3.694074, Jtr_pred = 0.179809, err = 0.053550, \u001b[31m   time: 8.023744 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089564, err = 0.026300\n",
      "\u001b[0m\n",
      "it 63/100, Jtr_KL = 3.685972, Jtr_pred = 0.182278, err = 0.054700, \u001b[31m   time: 8.016806 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091878, err = 0.028200\n",
      "\u001b[0m\n",
      "it 64/100, Jtr_KL = 3.677736, Jtr_pred = 0.180191, err = 0.052817, \u001b[31m   time: 8.465089 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.094015, err = 0.028500\n",
      "\u001b[0m\n",
      "it 65/100, Jtr_KL = 3.670278, Jtr_pred = 0.178788, err = 0.052333, \u001b[31m   time: 8.231186 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093302, err = 0.029300\n",
      "\u001b[0m\n",
      "it 66/100, Jtr_KL = 3.661878, Jtr_pred = 0.178021, err = 0.052033, \u001b[31m   time: 8.350740 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086590, err = 0.025900\n",
      "\u001b[0m\n",
      "it 67/100, Jtr_KL = 3.654423, Jtr_pred = 0.180254, err = 0.052950, \u001b[31m   time: 7.882750 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.092623, err = 0.028300\n",
      "\u001b[0m\n",
      "it 68/100, Jtr_KL = 3.646881, Jtr_pred = 0.179908, err = 0.053633, \u001b[31m   time: 8.072824 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.092226, err = 0.029600\n",
      "\u001b[0m\n",
      "it 69/100, Jtr_KL = 3.639274, Jtr_pred = 0.180421, err = 0.053067, \u001b[31m   time: 8.370627 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089049, err = 0.027900\n",
      "\u001b[0m\n",
      "it 70/100, Jtr_KL = 3.631913, Jtr_pred = 0.179358, err = 0.052350, \u001b[31m   time: 8.495262 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091587, err = 0.028600\n",
      "\u001b[0m\n",
      "it 71/100, Jtr_KL = 3.624507, Jtr_pred = 0.179505, err = 0.053267, \u001b[31m   time: 7.839555 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089910, err = 0.027900\n",
      "\u001b[0m\n",
      "it 72/100, Jtr_KL = 3.616737, Jtr_pred = 0.177907, err = 0.052733, \u001b[31m   time: 8.384600 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091068, err = 0.027400\n",
      "\u001b[0m\n",
      "it 73/100, Jtr_KL = 3.608695, Jtr_pred = 0.177523, err = 0.053150, \u001b[31m   time: 7.844444 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.087058, err = 0.027500\n",
      "\u001b[0m\n",
      "it 74/100, Jtr_KL = 3.601276, Jtr_pred = 0.177972, err = 0.052050, \u001b[31m   time: 8.322464 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.098658, err = 0.031100\n",
      "\u001b[0m\n",
      "it 75/100, Jtr_KL = 3.593640, Jtr_pred = 0.178934, err = 0.053483, \u001b[31m   time: 8.299730 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088520, err = 0.027300\n",
      "\u001b[0m\n",
      "it 76/100, Jtr_KL = 3.586819, Jtr_pred = 0.178085, err = 0.052367, \u001b[31m   time: 8.234540 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093775, err = 0.028300\n",
      "\u001b[0m\n",
      "it 77/100, Jtr_KL = 3.578729, Jtr_pred = 0.174912, err = 0.051517, \u001b[31m   time: 7.880122 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.092214, err = 0.028400\n",
      "\u001b[0m\n",
      "it 78/100, Jtr_KL = 3.571107, Jtr_pred = 0.174917, err = 0.051033, \u001b[31m   time: 8.172039 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086494, err = 0.026300\n",
      "\u001b[0m\n",
      "it 79/100, Jtr_KL = 3.563443, Jtr_pred = 0.175538, err = 0.051933, \u001b[31m   time: 8.194232 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.096708, err = 0.029600\n",
      "\u001b[0m\n",
      "it 80/100, Jtr_KL = 3.556307, Jtr_pred = 0.175287, err = 0.052183, \u001b[31m   time: 7.917948 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083564, err = 0.026100\n",
      "\u001b[0m\n",
      "it 81/100, Jtr_KL = 3.549041, Jtr_pred = 0.175354, err = 0.051567, \u001b[31m   time: 8.200861 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088601, err = 0.027600\n",
      "\u001b[0m\n",
      "it 82/100, Jtr_KL = 3.541393, Jtr_pred = 0.174691, err = 0.051650, \u001b[31m   time: 8.147404 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.085528, err = 0.027500\n",
      "\u001b[0m\n",
      "it 83/100, Jtr_KL = 3.534082, Jtr_pred = 0.176342, err = 0.051450, \u001b[31m   time: 7.971111 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090858, err = 0.029000\n",
      "\u001b[0m\n",
      "it 84/100, Jtr_KL = 3.526500, Jtr_pred = 0.173478, err = 0.051133, \u001b[31m   time: 8.098198 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.089956, err = 0.028100\n",
      "\u001b[0m\n",
      "it 85/100, Jtr_KL = 3.518811, Jtr_pred = 0.172010, err = 0.051417, \u001b[31m   time: 8.100153 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093691, err = 0.029200\n",
      "\u001b[0m\n",
      "it 86/100, Jtr_KL = 3.511339, Jtr_pred = 0.174564, err = 0.052883, \u001b[31m   time: 8.320800 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.094754, err = 0.030200\n",
      "\u001b[0m\n",
      "it 87/100, Jtr_KL = 3.503976, Jtr_pred = 0.174348, err = 0.052567, \u001b[31m   time: 7.970057 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.093308, err = 0.028300\n",
      "\u001b[0m\n",
      "it 88/100, Jtr_KL = 3.496915, Jtr_pred = 0.173714, err = 0.051100, \u001b[31m   time: 7.889568 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091865, err = 0.028100\n",
      "\u001b[0m\n",
      "it 89/100, Jtr_KL = 3.489513, Jtr_pred = 0.172670, err = 0.051367, \u001b[31m   time: 8.217783 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.090182, err = 0.027000\n",
      "\u001b[0m\n",
      "it 90/100, Jtr_KL = 3.481755, Jtr_pred = 0.173219, err = 0.051967, \u001b[31m   time: 8.290197 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.106131, err = 0.034000\n",
      "\u001b[0m\n",
      "it 91/100, Jtr_KL = 3.474660, Jtr_pred = 0.172338, err = 0.051150, \u001b[31m   time: 8.276677 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.086765, err = 0.026200\n",
      "\u001b[0m\n",
      "it 92/100, Jtr_KL = 3.467097, Jtr_pred = 0.173541, err = 0.052417, \u001b[31m   time: 8.235124 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.084490, err = 0.025800\n",
      "\u001b[0m\n",
      "it 93/100, Jtr_KL = 3.459606, Jtr_pred = 0.173943, err = 0.051283, \u001b[31m   time: 8.250797 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.083616, err = 0.025500\n",
      "\u001b[0m\n",
      "it 94/100, Jtr_KL = 3.452935, Jtr_pred = 0.173043, err = 0.050667, \u001b[31m   time: 8.229446 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088198, err = 0.026800\n",
      "\u001b[0m\n",
      "it 95/100, Jtr_KL = 3.445697, Jtr_pred = 0.171812, err = 0.050783, \u001b[31m   time: 8.282601 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.097615, err = 0.028600\n",
      "\u001b[0m\n",
      "it 96/100, Jtr_KL = 3.438481, Jtr_pred = 0.172111, err = 0.051783, \u001b[31m   time: 8.217173 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.091443, err = 0.028100\n",
      "\u001b[0m\n",
      "it 97/100, Jtr_KL = 3.431421, Jtr_pred = 0.170934, err = 0.050600, \u001b[31m   time: 8.252443 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.088242, err = 0.028000\n",
      "\u001b[0m\n",
      "it 98/100, Jtr_KL = 3.424005, Jtr_pred = 0.171767, err = 0.051650, \u001b[31m   time: 8.543402 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.092746, err = 0.028200\n",
      "\u001b[0m\n",
      "it 99/100, Jtr_KL = 3.417073, Jtr_pred = 0.172552, err = 0.050083, \u001b[31m   time: 7.888710 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.085645, err = 0.027100\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m   average time: 8.882693 seconds\n",
      "\u001b[0m\n",
      "\u001b[36mWritting bbb/theta_last.dat\n",
      "\u001b[0m\n",
      "\u001b[36m\n",
      "RESULTS:\u001b[0m\n",
      "  cost_dev: 0.083564 (cost_train 0.170934)\n",
      "  err_dev: 0.025500\n",
      "  nb_parameters: 4790420 (4.57MB)\n",
      "  time_per_it: 8.882693s\n",
      "\n",
      "snr: 2395210\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_dir = 'bbb'\n",
    "results_dir = 'bbb'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "NTrainPointsMNIST = 60000\n",
    "batch_size = 128\n",
    "nb_epochs = 100\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-3\n",
    "nsamples = int(3)  # How many samples to estimate ELBO with at each iteration\n",
    "\n",
    "# if True: #args.model == 'Local_Reparam':\n",
    "net = BBP_Bayes_Net_LR(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n",
    "                 Nbatches=(NTrainPointsMNIST / batch_size), nhid=1200, prior_sig=0.1)#args.prior_sig)\n",
    "\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "epoch = 0\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "    # We draw more samples on the first epoch in order to ensure convergence\n",
    "    if i == 0:\n",
    "        ELBO_samples = 10\n",
    "    else:\n",
    "        ELBO_samples = nsamples\n",
    "\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost_dkl, cost_pred, err = net.fit(x, y, samples=ELBO_samples)\n",
    "\n",
    "        err_train[i] += err\n",
    "        kl_cost_train[i] += cost_dkl\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    kl_cost_train[i] /= nb_samples  # Normalise by number of samples in order to get comparable number to the -log like\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_KL = %f, Jtr_pred = %f, err = %f, \" % (\n",
    "    i, nb_epochs, kl_cost_train[i], pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            cost, err, probs = net.eval(x, y)  # This takes the expected weights to save time, not proper inference\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir + '/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "net.save(models_dir + '/theta_last.dat')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "## Save results for plots\n",
    "# np.save('results/test_predictions.npy', test_predictions)\n",
    "np.save(results_dir + '/KL_cost_train.npy', kl_cost_train)\n",
    "np.save(results_dir + '/pred_cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "torch.save(net, 'bbb.net')\n",
    "\n",
    "%matplotlib inline\n",
    "snr = net.get_weight_SNR()\n",
    "print('snr:', len(snr))\n",
    "plt.hist(snr, bins=np.linspace(-0.05, 0.15, 500))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199210\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(net.get_weight_samples(), open('results/BBBweights.pkl','wb'))\n",
    "\n",
    "%matplotlib inline\n",
    "snr = net.get_weight_SNR()\n",
    "\n",
    "print(len(snr))\n",
    "plt.title('Signal to Noise Ratio of Weights')\n",
    "plt.hist(snr, bins=np.linspace(-0.01, 0.35, 250), density=True, alpha=0.6)\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Absolute Value of Signal to Noise Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\n",
      "Data:\u001b[0m\n",
      "\u001b[36m\n",
      "Network:\u001b[0m\n",
      "\u001b[36m\n",
      "Net:\u001b[0m\n",
      "\u001b[33m Creating Net!! \u001b[0m\n",
      "    Total params: 2.40M\n",
      "\u001b[36m\n",
      "Train:\u001b[0m\n",
      "  init cost variables:\n",
      "it 0/100, Jtr_pred = 0.329998, err = 0.099750, \u001b[31m   time: 2.821560 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.201081, err = 0.060200\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 1/100, Jtr_pred = 0.188278, err = 0.056350, \u001b[31m   time: 2.782595 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.172069, err = 0.050800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 2/100, Jtr_pred = 0.168964, err = 0.049300, \u001b[31m   time: 2.786068 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152290, err = 0.042100\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 3/100, Jtr_pred = 0.164831, err = 0.047517, \u001b[31m   time: 2.830179 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.153154, err = 0.046900\n",
      "\u001b[0m\n",
      "it 4/100, Jtr_pred = 0.159371, err = 0.044633, \u001b[31m   time: 2.735301 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144527, err = 0.041800\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 5/100, Jtr_pred = 0.157838, err = 0.045050, \u001b[31m   time: 2.792760 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144865, err = 0.043500\n",
      "\u001b[0m\n",
      "it 6/100, Jtr_pred = 0.156082, err = 0.045600, \u001b[31m   time: 2.730515 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142211, err = 0.041300\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 7/100, Jtr_pred = 0.155149, err = 0.044250, \u001b[31m   time: 2.786064 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.158750, err = 0.050800\n",
      "\u001b[0m\n",
      "it 8/100, Jtr_pred = 0.156329, err = 0.044150, \u001b[31m   time: 2.753433 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.151070, err = 0.042800\n",
      "\u001b[0m\n",
      "it 9/100, Jtr_pred = 0.155946, err = 0.044000, \u001b[31m   time: 2.708952 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.148617, err = 0.044500\n",
      "\u001b[0m\n",
      "it 10/100, Jtr_pred = 0.154556, err = 0.043650, \u001b[31m   time: 2.716593 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.140475, err = 0.038900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 11/100, Jtr_pred = 0.152281, err = 0.043550, \u001b[31m   time: 2.761398 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.163583, err = 0.047400\n",
      "\u001b[0m\n",
      "it 12/100, Jtr_pred = 0.155917, err = 0.044017, \u001b[31m   time: 2.750506 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144514, err = 0.040200\n",
      "\u001b[0m\n",
      "it 13/100, Jtr_pred = 0.154931, err = 0.045067, \u001b[31m   time: 2.829053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.150868, err = 0.043100\n",
      "\u001b[0m\n",
      "it 14/100, Jtr_pred = 0.152245, err = 0.043150, \u001b[31m   time: 2.763408 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.166535, err = 0.051400\n",
      "\u001b[0m\n",
      "it 15/100, Jtr_pred = 0.153101, err = 0.043833, \u001b[31m   time: 2.792235 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.173231, err = 0.052200\n",
      "\u001b[0m\n",
      "it 16/100, Jtr_pred = 0.151850, err = 0.042883, \u001b[31m   time: 2.818797 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.143766, err = 0.040500\n",
      "\u001b[0m\n",
      "it 17/100, Jtr_pred = 0.153433, err = 0.044017, \u001b[31m   time: 2.799951 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.146862, err = 0.043100\n",
      "\u001b[0m\n",
      "it 18/100, Jtr_pred = 0.153053, err = 0.043567, \u001b[31m   time: 2.845442 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.153362, err = 0.043500\n",
      "\u001b[0m\n",
      "it 19/100, Jtr_pred = 0.154006, err = 0.043800, \u001b[31m   time: 2.754490 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142531, err = 0.041600\n",
      "\u001b[0m\n",
      "it 20/100, Jtr_pred = 0.152098, err = 0.044183, \u001b[31m   time: 2.734801 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.141003, err = 0.039500\n",
      "\u001b[0m\n",
      "it 21/100, Jtr_pred = 0.152634, err = 0.042950, \u001b[31m   time: 2.815399 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.160330, err = 0.048500\n",
      "\u001b[0m\n",
      "it 22/100, Jtr_pred = 0.152169, err = 0.044083, \u001b[31m   time: 2.841635 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.160454, err = 0.050200\n",
      "\u001b[0m\n",
      "it 23/100, Jtr_pred = 0.152059, err = 0.043700, \u001b[31m   time: 2.722613 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.153631, err = 0.043500\n",
      "\u001b[0m\n",
      "it 24/100, Jtr_pred = 0.153192, err = 0.043050, \u001b[31m   time: 2.751938 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.147184, err = 0.039800\n",
      "\u001b[0m\n",
      "it 25/100, Jtr_pred = 0.153244, err = 0.044233, \u001b[31m   time: 2.722863 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.175159, err = 0.053700\n",
      "\u001b[0m\n",
      "it 26/100, Jtr_pred = 0.152798, err = 0.043350, \u001b[31m   time: 2.740291 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.149836, err = 0.042900\n",
      "\u001b[0m\n",
      "it 27/100, Jtr_pred = 0.151615, err = 0.043167, \u001b[31m   time: 2.762292 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.139620, err = 0.038400\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 28/100, Jtr_pred = 0.152648, err = 0.043617, \u001b[31m   time: 2.854152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.161721, err = 0.048600\n",
      "\u001b[0m\n",
      "it 29/100, Jtr_pred = 0.152680, err = 0.044183, \u001b[31m   time: 2.827363 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142386, err = 0.040100\n",
      "\u001b[0m\n",
      "it 30/100, Jtr_pred = 0.152111, err = 0.043467, \u001b[31m   time: 2.778131 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152545, err = 0.043100\n",
      "\u001b[0m\n",
      "it 31/100, Jtr_pred = 0.151550, err = 0.042933, \u001b[31m   time: 2.717345 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.150262, err = 0.042400\n",
      "\u001b[0m\n",
      "it 32/100, Jtr_pred = 0.153816, err = 0.044017, \u001b[31m   time: 2.760581 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.158128, err = 0.045200\n",
      "\u001b[0m\n",
      "it 33/100, Jtr_pred = 0.152300, err = 0.043517, \u001b[31m   time: 2.761238 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.174864, err = 0.054100\n",
      "\u001b[0m\n",
      "it 34/100, Jtr_pred = 0.155310, err = 0.044800, \u001b[31m   time: 2.836636 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.143697, err = 0.043800\n",
      "\u001b[0m\n",
      "it 35/100, Jtr_pred = 0.152149, err = 0.043533, \u001b[31m   time: 2.776532 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.145920, err = 0.042300\n",
      "\u001b[0m\n",
      "it 36/100, Jtr_pred = 0.151887, err = 0.042933, \u001b[31m   time: 2.799152 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152393, err = 0.045300\n",
      "\u001b[0m\n",
      "it 37/100, Jtr_pred = 0.153317, err = 0.043783, \u001b[31m   time: 2.840892 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.154575, err = 0.046300\n",
      "\u001b[0m\n",
      "it 38/100, Jtr_pred = 0.151559, err = 0.043150, \u001b[31m   time: 2.770816 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.141839, err = 0.041600\n",
      "\u001b[0m\n",
      "it 39/100, Jtr_pred = 0.152034, err = 0.043350, \u001b[31m   time: 2.720826 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.175007, err = 0.052500\n",
      "\u001b[0m\n",
      "it 40/100, Jtr_pred = 0.153449, err = 0.042633, \u001b[31m   time: 2.751827 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.154183, err = 0.044700\n",
      "\u001b[0m\n",
      "it 41/100, Jtr_pred = 0.152921, err = 0.043517, \u001b[31m   time: 2.746079 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152807, err = 0.045400\n",
      "\u001b[0m\n",
      "it 42/100, Jtr_pred = 0.152489, err = 0.044667, \u001b[31m   time: 2.712563 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.150396, err = 0.042400\n",
      "\u001b[0m\n",
      "it 43/100, Jtr_pred = 0.151152, err = 0.043233, \u001b[31m   time: 2.836057 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.143103, err = 0.040700\n",
      "\u001b[0m\n",
      "it 44/100, Jtr_pred = 0.152042, err = 0.043283, \u001b[31m   time: 2.883776 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.172912, err = 0.050900\n",
      "\u001b[0m\n",
      "it 45/100, Jtr_pred = 0.152587, err = 0.043817, \u001b[31m   time: 2.770612 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144596, err = 0.041500\n",
      "\u001b[0m\n",
      "it 46/100, Jtr_pred = 0.152300, err = 0.043733, \u001b[31m   time: 2.810809 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.140386, err = 0.041100\n",
      "\u001b[0m\n",
      "it 47/100, Jtr_pred = 0.153315, err = 0.043483, \u001b[31m   time: 2.768843 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.159953, err = 0.048400\n",
      "\u001b[0m\n",
      "it 48/100, Jtr_pred = 0.151198, err = 0.042850, \u001b[31m   time: 2.860634 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152242, err = 0.044500\n",
      "\u001b[0m\n",
      "it 49/100, Jtr_pred = 0.151821, err = 0.044317, \u001b[31m   time: 2.823183 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.160199, err = 0.047300\n",
      "\u001b[0m\n",
      "it 50/100, Jtr_pred = 0.151611, err = 0.044533, \u001b[31m   time: 2.714812 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.148348, err = 0.041200\n",
      "\u001b[0m\n",
      "it 51/100, Jtr_pred = 0.154833, err = 0.045100, \u001b[31m   time: 2.762158 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.143078, err = 0.040500\n",
      "\u001b[0m\n",
      "it 52/100, Jtr_pred = 0.152764, err = 0.043800, \u001b[31m   time: 2.741874 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.139400, err = 0.039000\n",
      "\u001b[0m\n",
      "it 53/100, Jtr_pred = 0.152529, err = 0.042817, \u001b[31m   time: 2.713769 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.155575, err = 0.045100\n",
      "\u001b[0m\n",
      "it 54/100, Jtr_pred = 0.152663, err = 0.043550, \u001b[31m   time: 2.744143 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142510, err = 0.037900\n",
      "\u001b[0m\n",
      "\u001b[34mbest test error\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_best.dat\n",
      "\u001b[0m\n",
      "it 55/100, Jtr_pred = 0.151886, err = 0.043367, \u001b[31m   time: 2.814263 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.148037, err = 0.042800\n",
      "\u001b[0m\n",
      "it 56/100, Jtr_pred = 0.152689, err = 0.044233, \u001b[31m   time: 2.769830 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.146720, err = 0.042000\n",
      "\u001b[0m\n",
      "it 57/100, Jtr_pred = 0.150749, err = 0.042700, \u001b[31m   time: 2.795396 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144813, err = 0.043100\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 58/100, Jtr_pred = 0.151731, err = 0.043533, \u001b[31m   time: 2.819460 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152191, err = 0.043700\n",
      "\u001b[0m\n",
      "it 59/100, Jtr_pred = 0.152084, err = 0.043317, \u001b[31m   time: 2.758585 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144703, err = 0.039000\n",
      "\u001b[0m\n",
      "it 60/100, Jtr_pred = 0.151814, err = 0.043783, \u001b[31m   time: 2.808435 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.139098, err = 0.039600\n",
      "\u001b[0m\n",
      "it 61/100, Jtr_pred = 0.151155, err = 0.043683, \u001b[31m   time: 2.759633 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144968, err = 0.041300\n",
      "\u001b[0m\n",
      "it 62/100, Jtr_pred = 0.152908, err = 0.044350, \u001b[31m   time: 2.832317 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.159467, err = 0.045600\n",
      "\u001b[0m\n",
      "it 63/100, Jtr_pred = 0.151994, err = 0.043883, \u001b[31m   time: 2.740132 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.151773, err = 0.043400\n",
      "\u001b[0m\n",
      "it 64/100, Jtr_pred = 0.152152, err = 0.043333, \u001b[31m   time: 2.735088 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.145927, err = 0.041400\n",
      "\u001b[0m\n",
      "it 65/100, Jtr_pred = 0.151778, err = 0.043600, \u001b[31m   time: 2.796052 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.154828, err = 0.041100\n",
      "\u001b[0m\n",
      "it 66/100, Jtr_pred = 0.154522, err = 0.043733, \u001b[31m   time: 2.784662 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144256, err = 0.042000\n",
      "\u001b[0m\n",
      "it 67/100, Jtr_pred = 0.152557, err = 0.043617, \u001b[31m   time: 2.715254 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.149622, err = 0.043100\n",
      "\u001b[0m\n",
      "it 68/100, Jtr_pred = 0.151680, err = 0.043283, \u001b[31m   time: 2.823090 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.139875, err = 0.039000\n",
      "\u001b[0m\n",
      "it 69/100, Jtr_pred = 0.152121, err = 0.043517, \u001b[31m   time: 2.755029 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.149032, err = 0.043000\n",
      "\u001b[0m\n",
      "it 70/100, Jtr_pred = 0.153148, err = 0.043933, \u001b[31m   time: 2.722566 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.148495, err = 0.043400\n",
      "\u001b[0m\n",
      "it 71/100, Jtr_pred = 0.153013, err = 0.043383, \u001b[31m   time: 2.725817 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.147065, err = 0.042500\n",
      "\u001b[0m\n",
      "it 72/100, Jtr_pred = 0.151833, err = 0.043533, \u001b[31m   time: 2.699305 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.149829, err = 0.042600\n",
      "\u001b[0m\n",
      "it 73/100, Jtr_pred = 0.152664, err = 0.042917, \u001b[31m   time: 2.758716 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152973, err = 0.046400\n",
      "\u001b[0m\n",
      "it 74/100, Jtr_pred = 0.151938, err = 0.043083, \u001b[31m   time: 2.699880 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.156515, err = 0.048000\n",
      "\u001b[0m\n",
      "it 75/100, Jtr_pred = 0.154189, err = 0.043850, \u001b[31m   time: 2.750631 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.151942, err = 0.043000\n",
      "\u001b[0m\n",
      "it 76/100, Jtr_pred = 0.151867, err = 0.043667, \u001b[31m   time: 2.756457 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.163549, err = 0.049200\n",
      "\u001b[0m\n",
      "it 77/100, Jtr_pred = 0.152175, err = 0.043383, \u001b[31m   time: 2.793722 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.139943, err = 0.039700\n",
      "\u001b[0m\n",
      "it 78/100, Jtr_pred = 0.151612, err = 0.042717, \u001b[31m   time: 2.800167 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.139314, err = 0.040600\n",
      "\u001b[0m\n",
      "it 79/100, Jtr_pred = 0.154149, err = 0.044017, \u001b[31m   time: 2.767694 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.146678, err = 0.039500\n",
      "\u001b[0m\n",
      "it 80/100, Jtr_pred = 0.152583, err = 0.044183, \u001b[31m   time: 2.733732 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.154575, err = 0.044200\n",
      "\u001b[0m\n",
      "it 81/100, Jtr_pred = 0.150919, err = 0.042383, \u001b[31m   time: 2.837467 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.167025, err = 0.049700\n",
      "\u001b[0m\n",
      "it 82/100, Jtr_pred = 0.150412, err = 0.043817, \u001b[31m   time: 2.751743 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.145715, err = 0.042100\n",
      "\u001b[0m\n",
      "it 83/100, Jtr_pred = 0.151594, err = 0.043500, \u001b[31m   time: 2.807193 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144968, err = 0.040300\n",
      "\u001b[0m\n",
      "it 84/100, Jtr_pred = 0.152286, err = 0.043450, \u001b[31m   time: 2.753345 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.140229, err = 0.040300\n",
      "\u001b[0m\n",
      "it 85/100, Jtr_pred = 0.153643, err = 0.043483, \u001b[31m   time: 2.740842 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.159098, err = 0.044800\n",
      "\u001b[0m\n",
      "it 86/100, Jtr_pred = 0.152545, err = 0.043533, \u001b[31m   time: 2.767740 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142162, err = 0.040000\n",
      "\u001b[0m\n",
      "it 87/100, Jtr_pred = 0.152617, err = 0.043150, \u001b[31m   time: 2.743133 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.165710, err = 0.048800\n",
      "\u001b[0m\n",
      "it 88/100, Jtr_pred = 0.152273, err = 0.043483, \u001b[31m   time: 2.751179 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.138477, err = 0.038200\n",
      "\u001b[0m\n",
      "it 89/100, Jtr_pred = 0.151840, err = 0.042900, \u001b[31m   time: 2.704534 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.197855, err = 0.060800\n",
      "\u001b[0m\n",
      "it 90/100, Jtr_pred = 0.152187, err = 0.043633, \u001b[31m   time: 2.719537 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.138770, err = 0.038900\n",
      "\u001b[0m\n",
      "it 91/100, Jtr_pred = 0.153548, err = 0.043683, \u001b[31m   time: 2.811493 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.155760, err = 0.044400\n",
      "\u001b[0m\n",
      "it 92/100, Jtr_pred = 0.152555, err = 0.042783, \u001b[31m   time: 2.739595 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.149945, err = 0.046000\n",
      "\u001b[0m\n",
      "it 93/100, Jtr_pred = 0.152337, err = 0.043150, \u001b[31m   time: 2.734502 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.144068, err = 0.041100\n",
      "\u001b[0m\n",
      "it 94/100, Jtr_pred = 0.151543, err = 0.043433, \u001b[31m   time: 2.786576 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.150056, err = 0.044300\n",
      "\u001b[0m\n",
      "it 95/100, Jtr_pred = 0.152464, err = 0.043367, \u001b[31m   time: 2.712758 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.155207, err = 0.041700\n",
      "\u001b[0m\n",
      "it 96/100, Jtr_pred = 0.152463, err = 0.043317, \u001b[31m   time: 2.763782 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.142883, err = 0.041800\n",
      "\u001b[0m\n",
      "it 97/100, Jtr_pred = 0.151171, err = 0.042933, \u001b[31m   time: 2.795685 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.153724, err = 0.043900\n",
      "\u001b[0m\n",
      "it 98/100, Jtr_pred = 0.153718, err = 0.044267, \u001b[31m   time: 2.817078 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.152171, err = 0.044700\n",
      "\u001b[0m\n",
      "it 99/100, Jtr_pred = 0.152491, err = 0.043917, \u001b[31m   time: 2.822834 seconds\n",
      "\u001b[0m\n",
      "\u001b[32m    Jdev = 0.134849, err = 0.039000\n",
      "\u001b[0m\n",
      "\u001b[31m   average time: 3.332587 seconds\n",
      "\u001b[0m\n",
      "\u001b[36mWritting dropout/theta_last.dat\n",
      "\u001b[0m\n",
      "\u001b[36m\n",
      "RESULTS:\u001b[0m\n",
      "  cost_dev: 0.134849 (cost_train 0.150412)\n",
      "  err_dev: 0.037900\n",
      "  nb_parameters: 2395210 (2.28MB)\n",
      "  time_per_it: 3.332587s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "from MC_dropout.model import *\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "models_dir = 'dropout'\n",
    "results_dir = 'dropout'\n",
    "\n",
    "mkdir(models_dir)\n",
    "mkdir(results_dir)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "NTrainPointsMNIST = 60000\n",
    "batch_size = 128\n",
    "nb_epochs = 100\n",
    "log_interval = 1\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# dataset\n",
    "cprint('c', '\\nData:')\n",
    "\n",
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "cprint('c', '\\nNetwork:')\n",
    "\n",
    "lr = 1e-3\n",
    "########################################################################################\n",
    "\n",
    "net = MC_drop_net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n",
    "                  weight_decay=1, n_hid=1200)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "epoch = 0\n",
    "cprint('c', '\\nTrain:')\n",
    "\n",
    "print('  init cost variables:')\n",
    "kl_cost_train = np.zeros(nb_epochs)\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "nb_its_dev = 1\n",
    "\n",
    "tic0 = time.time()\n",
    "for i in range(epoch, nb_epochs):\n",
    "\n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        cost_pred, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    cprint('r', '   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "\n",
    "        cprint('g', '    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            cprint('b', 'best test error')\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "cprint('r', '   average time: %f seconds\\n' % runtime_per_it)\n",
    "\n",
    "net.save(models_dir+'/theta_last.dat')\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# results\n",
    "cprint('c', '\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "## Save results for plots\n",
    "np.save(results_dir + '/cost_train.npy', pred_cost_train)\n",
    "np.save(results_dir + '/cost_dev.npy', cost_dev)\n",
    "np.save(results_dir + '/err_train.npy', err_train)\n",
    "np.save(results_dir + '/err_dev.npy', err_dev)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_weights = net.get_weight_samples()\n",
    "pickle.dump(dropout_weights, open('results/dropoutweights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "#         self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "nhid = 400\n",
    "net = Net(784, nhid, 10)\n",
    "net.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/468], Loss: 0.1897\n",
      "Epoch [1/100], Step [200/468], Loss: 0.1501\n",
      "Epoch [1/100], Step [300/468], Loss: 0.0732\n",
      "Epoch [1/100], Step [400/468], Loss: 0.0961\n",
      "Epoch [2/100], Step [100/468], Loss: 0.1036\n",
      "Epoch [2/100], Step [200/468], Loss: 0.0318\n",
      "Epoch [2/100], Step [300/468], Loss: 0.1398\n",
      "Epoch [2/100], Step [400/468], Loss: 0.0827\n",
      "Epoch [3/100], Step [100/468], Loss: 0.1262\n",
      "Epoch [3/100], Step [200/468], Loss: 0.0540\n",
      "Epoch [3/100], Step [300/468], Loss: 0.0742\n",
      "Epoch [3/100], Step [400/468], Loss: 0.0426\n",
      "Epoch [4/100], Step [100/468], Loss: 0.0422\n",
      "Epoch [4/100], Step [200/468], Loss: 0.0516\n",
      "Epoch [4/100], Step [300/468], Loss: 0.0756\n",
      "Epoch [4/100], Step [400/468], Loss: 0.0574\n",
      "Epoch [5/100], Step [100/468], Loss: 0.0148\n",
      "Epoch [5/100], Step [200/468], Loss: 0.0461\n",
      "Epoch [5/100], Step [300/468], Loss: 0.0055\n",
      "Epoch [5/100], Step [400/468], Loss: 0.0175\n",
      "Epoch [6/100], Step [100/468], Loss: 0.0802\n",
      "Epoch [6/100], Step [200/468], Loss: 0.0495\n",
      "Epoch [6/100], Step [300/468], Loss: 0.0289\n",
      "Epoch [6/100], Step [400/468], Loss: 0.0203\n",
      "Epoch [7/100], Step [100/468], Loss: 0.0048\n",
      "Epoch [7/100], Step [200/468], Loss: 0.0032\n",
      "Epoch [7/100], Step [300/468], Loss: 0.0085\n",
      "Epoch [7/100], Step [400/468], Loss: 0.0061\n",
      "Epoch [8/100], Step [100/468], Loss: 0.0023\n",
      "Epoch [8/100], Step [200/468], Loss: 0.0052\n",
      "Epoch [8/100], Step [300/468], Loss: 0.0286\n",
      "Epoch [8/100], Step [400/468], Loss: 0.0133\n",
      "Epoch [9/100], Step [100/468], Loss: 0.0024\n",
      "Epoch [9/100], Step [200/468], Loss: 0.0502\n",
      "Epoch [9/100], Step [300/468], Loss: 0.0080\n",
      "Epoch [9/100], Step [400/468], Loss: 0.0347\n",
      "Epoch [10/100], Step [100/468], Loss: 0.0022\n",
      "Epoch [10/100], Step [200/468], Loss: 0.0063\n",
      "Epoch [10/100], Step [300/468], Loss: 0.0011\n",
      "Epoch [10/100], Step [400/468], Loss: 0.0089\n",
      "Epoch [11/100], Step [100/468], Loss: 0.0230\n",
      "Epoch [11/100], Step [200/468], Loss: 0.0099\n",
      "Epoch [11/100], Step [300/468], Loss: 0.0126\n",
      "Epoch [11/100], Step [400/468], Loss: 0.1374\n",
      "Epoch [12/100], Step [100/468], Loss: 0.0084\n",
      "Epoch [12/100], Step [200/468], Loss: 0.0130\n",
      "Epoch [12/100], Step [300/468], Loss: 0.0005\n",
      "Epoch [12/100], Step [400/468], Loss: 0.0076\n",
      "Epoch [13/100], Step [100/468], Loss: 0.0074\n",
      "Epoch [13/100], Step [200/468], Loss: 0.0017\n",
      "Epoch [13/100], Step [300/468], Loss: 0.0012\n",
      "Epoch [13/100], Step [400/468], Loss: 0.0157\n",
      "Epoch [14/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [14/100], Step [200/468], Loss: 0.0031\n",
      "Epoch [14/100], Step [300/468], Loss: 0.0167\n",
      "Epoch [14/100], Step [400/468], Loss: 0.0124\n",
      "Epoch [15/100], Step [100/468], Loss: 0.0035\n",
      "Epoch [15/100], Step [200/468], Loss: 0.0095\n",
      "Epoch [15/100], Step [300/468], Loss: 0.0093\n",
      "Epoch [15/100], Step [400/468], Loss: 0.0194\n",
      "Epoch [16/100], Step [100/468], Loss: 0.0123\n",
      "Epoch [16/100], Step [200/468], Loss: 0.0171\n",
      "Epoch [16/100], Step [300/468], Loss: 0.0014\n",
      "Epoch [16/100], Step [400/468], Loss: 0.0081\n",
      "Epoch [17/100], Step [100/468], Loss: 0.0007\n",
      "Epoch [17/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [17/100], Step [300/468], Loss: 0.0012\n",
      "Epoch [17/100], Step [400/468], Loss: 0.0011\n",
      "Epoch [18/100], Step [100/468], Loss: 0.0116\n",
      "Epoch [18/100], Step [200/468], Loss: 0.0005\n",
      "Epoch [18/100], Step [300/468], Loss: 0.0063\n",
      "Epoch [18/100], Step [400/468], Loss: 0.0425\n",
      "Epoch [19/100], Step [100/468], Loss: 0.0010\n",
      "Epoch [19/100], Step [200/468], Loss: 0.0027\n",
      "Epoch [19/100], Step [300/468], Loss: 0.0697\n",
      "Epoch [19/100], Step [400/468], Loss: 0.0854\n",
      "Epoch [20/100], Step [100/468], Loss: 0.0007\n",
      "Epoch [20/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [20/100], Step [300/468], Loss: 0.0181\n",
      "Epoch [20/100], Step [400/468], Loss: 0.0016\n",
      "Epoch [21/100], Step [100/468], Loss: 0.0013\n",
      "Epoch [21/100], Step [200/468], Loss: 0.0002\n",
      "Epoch [21/100], Step [300/468], Loss: 0.0068\n",
      "Epoch [21/100], Step [400/468], Loss: 0.0027\n",
      "Epoch [22/100], Step [100/468], Loss: 0.0201\n",
      "Epoch [22/100], Step [200/468], Loss: 0.0225\n",
      "Epoch [22/100], Step [300/468], Loss: 0.0171\n",
      "Epoch [22/100], Step [400/468], Loss: 0.0024\n",
      "Epoch [23/100], Step [100/468], Loss: 0.0012\n",
      "Epoch [23/100], Step [200/468], Loss: 0.0020\n",
      "Epoch [23/100], Step [300/468], Loss: 0.0027\n",
      "Epoch [23/100], Step [400/468], Loss: 0.0018\n",
      "Epoch [24/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [24/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [24/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [24/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [25/100], Step [100/468], Loss: 0.0076\n",
      "Epoch [25/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [25/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [25/100], Step [400/468], Loss: 0.0029\n",
      "Epoch [26/100], Step [100/468], Loss: 0.0005\n",
      "Epoch [26/100], Step [200/468], Loss: 0.0007\n",
      "Epoch [26/100], Step [300/468], Loss: 0.0078\n",
      "Epoch [26/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [27/100], Step [100/468], Loss: 0.0236\n",
      "Epoch [27/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [27/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [27/100], Step [400/468], Loss: 0.0108\n",
      "Epoch [28/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [28/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [28/100], Step [300/468], Loss: 0.0013\n",
      "Epoch [28/100], Step [400/468], Loss: 0.0069\n",
      "Epoch [29/100], Step [100/468], Loss: 0.0074\n",
      "Epoch [29/100], Step [200/468], Loss: 0.0304\n",
      "Epoch [29/100], Step [300/468], Loss: 0.0112\n",
      "Epoch [29/100], Step [400/468], Loss: 0.0153\n",
      "Epoch [30/100], Step [100/468], Loss: 0.0036\n",
      "Epoch [30/100], Step [200/468], Loss: 0.0304\n",
      "Epoch [30/100], Step [300/468], Loss: 0.0019\n",
      "Epoch [30/100], Step [400/468], Loss: 0.0018\n",
      "Epoch [31/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [31/100], Step [200/468], Loss: 0.0009\n",
      "Epoch [31/100], Step [300/468], Loss: 0.0027\n",
      "Epoch [31/100], Step [400/468], Loss: 0.0242\n",
      "Epoch [32/100], Step [100/468], Loss: 0.0060\n",
      "Epoch [32/100], Step [200/468], Loss: 0.0041\n",
      "Epoch [32/100], Step [300/468], Loss: 0.0094\n",
      "Epoch [32/100], Step [400/468], Loss: 0.0328\n",
      "Epoch [33/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [33/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [33/100], Step [300/468], Loss: 0.0294\n",
      "Epoch [33/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [34/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [34/100], Step [200/468], Loss: 0.0033\n",
      "Epoch [34/100], Step [300/468], Loss: 0.0038\n",
      "Epoch [34/100], Step [400/468], Loss: 0.0014\n",
      "Epoch [35/100], Step [100/468], Loss: 0.0003\n",
      "Epoch [35/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [35/100], Step [300/468], Loss: 0.0068\n",
      "Epoch [35/100], Step [400/468], Loss: 0.0007\n",
      "Epoch [36/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [36/100], Step [200/468], Loss: 0.0178\n",
      "Epoch [36/100], Step [300/468], Loss: 0.0005\n",
      "Epoch [36/100], Step [400/468], Loss: 0.0015\n",
      "Epoch [37/100], Step [100/468], Loss: 0.0026\n",
      "Epoch [37/100], Step [200/468], Loss: 0.0026\n",
      "Epoch [37/100], Step [300/468], Loss: 0.0057\n",
      "Epoch [37/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [38/100], Step [100/468], Loss: 0.0069\n",
      "Epoch [38/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [38/100], Step [300/468], Loss: 0.0028\n",
      "Epoch [38/100], Step [400/468], Loss: 0.0253\n",
      "Epoch [39/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [39/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [39/100], Step [300/468], Loss: 0.0077\n",
      "Epoch [39/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [40/100], Step [100/468], Loss: 0.0054\n",
      "Epoch [40/100], Step [200/468], Loss: 0.0171\n",
      "Epoch [40/100], Step [300/468], Loss: 0.0014\n",
      "Epoch [40/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [41/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [41/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [41/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [41/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [42/100], Step [100/468], Loss: 0.0003\n",
      "Epoch [42/100], Step [200/468], Loss: 0.0065\n",
      "Epoch [42/100], Step [300/468], Loss: 0.0011\n",
      "Epoch [42/100], Step [400/468], Loss: 0.0008\n",
      "Epoch [43/100], Step [100/468], Loss: 0.0008\n",
      "Epoch [43/100], Step [200/468], Loss: 0.0007\n",
      "Epoch [43/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [43/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [44/100], Step [100/468], Loss: 0.0055\n",
      "Epoch [44/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [44/100], Step [300/468], Loss: 0.0014\n",
      "Epoch [44/100], Step [400/468], Loss: 0.0015\n",
      "Epoch [45/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [45/100], Step [200/468], Loss: 0.0171\n",
      "Epoch [45/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [45/100], Step [400/468], Loss: 0.0169\n",
      "Epoch [46/100], Step [100/468], Loss: 0.0180\n",
      "Epoch [46/100], Step [200/468], Loss: 0.0002\n",
      "Epoch [46/100], Step [300/468], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [47/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [47/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [47/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [47/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [48/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [48/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [48/100], Step [300/468], Loss: 0.0089\n",
      "Epoch [48/100], Step [400/468], Loss: 0.0016\n",
      "Epoch [49/100], Step [100/468], Loss: 0.0013\n",
      "Epoch [49/100], Step [200/468], Loss: 0.0159\n",
      "Epoch [49/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [49/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [200/468], Loss: 0.0469\n",
      "Epoch [50/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [51/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [51/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [52/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [52/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [52/100], Step [300/468], Loss: 0.0005\n",
      "Epoch [52/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [53/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [53/100], Step [200/468], Loss: 0.0019\n",
      "Epoch [53/100], Step [300/468], Loss: 0.0004\n",
      "Epoch [53/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [54/100], Step [100/468], Loss: 0.0009\n",
      "Epoch [54/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [54/100], Step [300/468], Loss: 0.0421\n",
      "Epoch [54/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [55/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [55/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [55/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [55/100], Step [400/468], Loss: 0.0589\n",
      "Epoch [56/100], Step [100/468], Loss: 0.0068\n",
      "Epoch [56/100], Step [200/468], Loss: 0.0129\n",
      "Epoch [56/100], Step [300/468], Loss: 0.0119\n",
      "Epoch [56/100], Step [400/468], Loss: 0.0007\n",
      "Epoch [57/100], Step [100/468], Loss: 0.0022\n",
      "Epoch [57/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [57/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [57/100], Step [400/468], Loss: 0.0006\n",
      "Epoch [58/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [58/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [58/100], Step [300/468], Loss: 0.0010\n",
      "Epoch [58/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [59/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [59/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [59/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [59/100], Step [400/468], Loss: 0.0007\n",
      "Epoch [60/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [60/100], Step [200/468], Loss: 0.0141\n",
      "Epoch [60/100], Step [300/468], Loss: 0.0041\n",
      "Epoch [60/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [61/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [61/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [61/100], Step [300/468], Loss: 0.0005\n",
      "Epoch [61/100], Step [400/468], Loss: 0.0447\n",
      "Epoch [62/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [62/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [62/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [62/100], Step [400/468], Loss: 0.0289\n",
      "Epoch [63/100], Step [100/468], Loss: 0.0016\n",
      "Epoch [63/100], Step [200/468], Loss: 0.0010\n",
      "Epoch [63/100], Step [300/468], Loss: 0.0005\n",
      "Epoch [63/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [64/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [64/100], Step [200/468], Loss: 0.0014\n",
      "Epoch [64/100], Step [300/468], Loss: 0.0007\n",
      "Epoch [64/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [65/100], Step [100/468], Loss: 0.0337\n",
      "Epoch [65/100], Step [200/468], Loss: 0.0044\n",
      "Epoch [65/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [65/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [66/100], Step [100/468], Loss: 0.0419\n",
      "Epoch [66/100], Step [200/468], Loss: 0.0008\n",
      "Epoch [66/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [66/100], Step [400/468], Loss: 0.0506\n",
      "Epoch [67/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [67/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [67/100], Step [300/468], Loss: 0.0004\n",
      "Epoch [67/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [100/468], Loss: 0.0393\n",
      "Epoch [68/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [68/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [100/468], Loss: 0.0671\n",
      "Epoch [69/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [300/468], Loss: 0.0311\n",
      "Epoch [69/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [70/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [70/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [71/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [300/468], Loss: 0.0122\n",
      "Epoch [71/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [200/468], Loss: 0.0006\n",
      "Epoch [72/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [400/468], Loss: 0.0036\n",
      "Epoch [73/100], Step [100/468], Loss: 0.0253\n",
      "Epoch [73/100], Step [200/468], Loss: 0.0025\n",
      "Epoch [73/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [73/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [74/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [74/100], Step [200/468], Loss: 0.0099\n",
      "Epoch [74/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [74/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [75/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [75/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [75/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [75/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [100/468], Loss: 0.0851\n",
      "Epoch [76/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [400/468], Loss: 0.0108\n",
      "Epoch [77/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [78/100], Step [200/468], Loss: 0.0012\n",
      "Epoch [78/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [400/468], Loss: 0.0019\n",
      "Epoch [79/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [80/100], Step [100/468], Loss: 0.0057\n",
      "Epoch [80/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [80/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [80/100], Step [400/468], Loss: 0.0026\n",
      "Epoch [81/100], Step [100/468], Loss: 0.0067\n",
      "Epoch [81/100], Step [200/468], Loss: 0.0005\n",
      "Epoch [81/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [81/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [83/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [83/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [83/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [83/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [84/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [400/468], Loss: 0.0399\n",
      "Epoch [85/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [85/100], Step [300/468], Loss: 0.0133\n",
      "Epoch [85/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [86/100], Step [100/468], Loss: 0.0009\n",
      "Epoch [86/100], Step [200/468], Loss: 0.0112\n",
      "Epoch [86/100], Step [300/468], Loss: 0.0823\n",
      "Epoch [86/100], Step [400/468], Loss: 0.0016\n",
      "Epoch [87/100], Step [100/468], Loss: 0.0150\n",
      "Epoch [87/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [87/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [87/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [88/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [88/100], Step [200/468], Loss: 0.0013\n",
      "Epoch [88/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [88/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [89/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [89/100], Step [200/468], Loss: 0.0003\n",
      "Epoch [89/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [89/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [90/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [400/468], Loss: 0.0555\n",
      "Epoch [91/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [91/100], Step [200/468], Loss: 0.0009\n",
      "Epoch [91/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [91/100], Step [400/468], Loss: 0.0665\n",
      "Epoch [92/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [92/100], Step [200/468], Loss: 0.0252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [92/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [93/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [93/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [93/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [93/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [94/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [400/468], Loss: 0.0056\n",
      "Epoch [96/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [96/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [96/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [96/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [97/100], Step [100/468], Loss: 0.0618\n",
      "Epoch [97/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [97/100], Step [300/468], Loss: 0.0004\n",
      "Epoch [97/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [300/468], Loss: 0.0066\n",
      "Epoch [98/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [99/100], Step [200/468], Loss: 0.0108\n",
      "Epoch [99/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [200/468], Loss: 0.0016\n",
      "Epoch [100/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [400/468], Loss: 0.0056\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size=128\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):   # Load a batch of images with its (index, data, class)\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images.cuda())                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels.cuda())                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0: # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, \n",
    "                   num_epochs, \n",
    "                   i+1, \n",
    "                   len(trainset)//batch_size, \n",
    "                   loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images:  98.12\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in valloader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images.cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels.cuda()).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on test images: ', 100 * correct.item() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/468], Loss: 0.2702\n",
      "Epoch [1/100], Step [200/468], Loss: 0.2146\n",
      "Epoch [1/100], Step [300/468], Loss: 0.1158\n",
      "Epoch [1/100], Step [400/468], Loss: 0.0927\n",
      "Epoch [2/100], Step [100/468], Loss: 0.0844\n",
      "Epoch [2/100], Step [200/468], Loss: 0.0471\n",
      "Epoch [2/100], Step [300/468], Loss: 0.0703\n",
      "Epoch [2/100], Step [400/468], Loss: 0.0861\n",
      "Epoch [3/100], Step [100/468], Loss: 0.0130\n",
      "Epoch [3/100], Step [200/468], Loss: 0.0142\n",
      "Epoch [3/100], Step [300/468], Loss: 0.0696\n",
      "Epoch [3/100], Step [400/468], Loss: 0.1232\n",
      "Epoch [4/100], Step [100/468], Loss: 0.0157\n",
      "Epoch [4/100], Step [200/468], Loss: 0.0827\n",
      "Epoch [4/100], Step [300/468], Loss: 0.0591\n",
      "Epoch [4/100], Step [400/468], Loss: 0.0947\n",
      "Epoch [5/100], Step [100/468], Loss: 0.0157\n",
      "Epoch [5/100], Step [200/468], Loss: 0.0060\n",
      "Epoch [5/100], Step [300/468], Loss: 0.0142\n",
      "Epoch [5/100], Step [400/468], Loss: 0.0336\n",
      "Epoch [6/100], Step [100/468], Loss: 0.0049\n",
      "Epoch [6/100], Step [200/468], Loss: 0.0296\n",
      "Epoch [6/100], Step [300/468], Loss: 0.0415\n",
      "Epoch [6/100], Step [400/468], Loss: 0.0597\n",
      "Epoch [7/100], Step [100/468], Loss: 0.0389\n",
      "Epoch [7/100], Step [200/468], Loss: 0.0241\n",
      "Epoch [7/100], Step [300/468], Loss: 0.0380\n",
      "Epoch [7/100], Step [400/468], Loss: 0.0129\n",
      "Epoch [8/100], Step [100/468], Loss: 0.0137\n",
      "Epoch [8/100], Step [200/468], Loss: 0.0808\n",
      "Epoch [8/100], Step [300/468], Loss: 0.0422\n",
      "Epoch [8/100], Step [400/468], Loss: 0.0075\n",
      "Epoch [9/100], Step [100/468], Loss: 0.0014\n",
      "Epoch [9/100], Step [200/468], Loss: 0.0105\n",
      "Epoch [9/100], Step [300/468], Loss: 0.0899\n",
      "Epoch [9/100], Step [400/468], Loss: 0.0361\n",
      "Epoch [10/100], Step [100/468], Loss: 0.0909\n",
      "Epoch [10/100], Step [200/468], Loss: 0.0405\n",
      "Epoch [10/100], Step [300/468], Loss: 0.0069\n",
      "Epoch [10/100], Step [400/468], Loss: 0.0216\n",
      "Epoch [11/100], Step [100/468], Loss: 0.0362\n",
      "Epoch [11/100], Step [200/468], Loss: 0.0108\n",
      "Epoch [11/100], Step [300/468], Loss: 0.0328\n",
      "Epoch [11/100], Step [400/468], Loss: 0.0436\n",
      "Epoch [12/100], Step [100/468], Loss: 0.0116\n",
      "Epoch [12/100], Step [200/468], Loss: 0.0359\n",
      "Epoch [12/100], Step [300/468], Loss: 0.0017\n",
      "Epoch [12/100], Step [400/468], Loss: 0.0094\n",
      "Epoch [13/100], Step [100/468], Loss: 0.0174\n",
      "Epoch [13/100], Step [200/468], Loss: 0.0122\n",
      "Epoch [13/100], Step [300/468], Loss: 0.0039\n",
      "Epoch [13/100], Step [400/468], Loss: 0.0248\n",
      "Epoch [14/100], Step [100/468], Loss: 0.0258\n",
      "Epoch [14/100], Step [200/468], Loss: 0.0012\n",
      "Epoch [14/100], Step [300/468], Loss: 0.0498\n",
      "Epoch [14/100], Step [400/468], Loss: 0.0253\n",
      "Epoch [15/100], Step [100/468], Loss: 0.0018\n",
      "Epoch [15/100], Step [200/468], Loss: 0.0124\n",
      "Epoch [15/100], Step [300/468], Loss: 0.0075\n",
      "Epoch [15/100], Step [400/468], Loss: 0.0280\n",
      "Epoch [16/100], Step [100/468], Loss: 0.0788\n",
      "Epoch [16/100], Step [200/468], Loss: 0.0068\n",
      "Epoch [16/100], Step [300/468], Loss: 0.0075\n",
      "Epoch [16/100], Step [400/468], Loss: 0.0035\n",
      "Epoch [17/100], Step [100/468], Loss: 0.0162\n",
      "Epoch [17/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [17/100], Step [300/468], Loss: 0.0393\n",
      "Epoch [17/100], Step [400/468], Loss: 0.0022\n",
      "Epoch [18/100], Step [100/468], Loss: 0.0072\n",
      "Epoch [18/100], Step [200/468], Loss: 0.0114\n",
      "Epoch [18/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [18/100], Step [400/468], Loss: 0.0229\n",
      "Epoch [19/100], Step [100/468], Loss: 0.0473\n",
      "Epoch [19/100], Step [200/468], Loss: 0.0604\n",
      "Epoch [19/100], Step [300/468], Loss: 0.0087\n",
      "Epoch [19/100], Step [400/468], Loss: 0.0121\n",
      "Epoch [20/100], Step [100/468], Loss: 0.0967\n",
      "Epoch [20/100], Step [200/468], Loss: 0.0874\n",
      "Epoch [20/100], Step [300/468], Loss: 0.0075\n",
      "Epoch [20/100], Step [400/468], Loss: 0.0165\n",
      "Epoch [21/100], Step [100/468], Loss: 0.0044\n",
      "Epoch [21/100], Step [200/468], Loss: 0.0225\n",
      "Epoch [21/100], Step [300/468], Loss: 0.0199\n",
      "Epoch [21/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [22/100], Step [100/468], Loss: 0.0007\n",
      "Epoch [22/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [22/100], Step [300/468], Loss: 0.0020\n",
      "Epoch [22/100], Step [400/468], Loss: 0.0005\n",
      "Epoch [23/100], Step [100/468], Loss: 0.0003\n",
      "Epoch [23/100], Step [200/468], Loss: 0.0175\n",
      "Epoch [23/100], Step [300/468], Loss: 0.0022\n",
      "Epoch [23/100], Step [400/468], Loss: 0.0052\n",
      "Epoch [24/100], Step [100/468], Loss: 0.0089\n",
      "Epoch [24/100], Step [200/468], Loss: 0.0092\n",
      "Epoch [24/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [24/100], Step [400/468], Loss: 0.0008\n",
      "Epoch [25/100], Step [100/468], Loss: 0.0557\n",
      "Epoch [25/100], Step [200/468], Loss: 0.0032\n",
      "Epoch [25/100], Step [300/468], Loss: 0.0852\n",
      "Epoch [25/100], Step [400/468], Loss: 0.0014\n",
      "Epoch [26/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [26/100], Step [200/468], Loss: 0.0180\n",
      "Epoch [26/100], Step [300/468], Loss: 0.0065\n",
      "Epoch [26/100], Step [400/468], Loss: 0.0019\n",
      "Epoch [27/100], Step [100/468], Loss: 0.0190\n",
      "Epoch [27/100], Step [200/468], Loss: 0.0002\n",
      "Epoch [27/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [27/100], Step [400/468], Loss: 0.0620\n",
      "Epoch [28/100], Step [100/468], Loss: 0.0028\n",
      "Epoch [28/100], Step [200/468], Loss: 0.0005\n",
      "Epoch [28/100], Step [300/468], Loss: 0.0186\n",
      "Epoch [28/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [29/100], Step [100/468], Loss: 0.0050\n",
      "Epoch [29/100], Step [200/468], Loss: 0.0258\n",
      "Epoch [29/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [29/100], Step [400/468], Loss: 0.0873\n",
      "Epoch [30/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [30/100], Step [200/468], Loss: 0.0037\n",
      "Epoch [30/100], Step [300/468], Loss: 0.0783\n",
      "Epoch [30/100], Step [400/468], Loss: 0.0007\n",
      "Epoch [31/100], Step [100/468], Loss: 0.0035\n",
      "Epoch [31/100], Step [200/468], Loss: 0.0022\n",
      "Epoch [31/100], Step [300/468], Loss: 0.0138\n",
      "Epoch [31/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [32/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [32/100], Step [200/468], Loss: 0.0051\n",
      "Epoch [32/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [32/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [33/100], Step [100/468], Loss: 0.0045\n",
      "Epoch [33/100], Step [200/468], Loss: 0.0010\n",
      "Epoch [33/100], Step [300/468], Loss: 0.0390\n",
      "Epoch [33/100], Step [400/468], Loss: 0.0010\n",
      "Epoch [34/100], Step [100/468], Loss: 0.0661\n",
      "Epoch [34/100], Step [200/468], Loss: 0.0109\n",
      "Epoch [34/100], Step [300/468], Loss: 0.0006\n",
      "Epoch [34/100], Step [400/468], Loss: 0.0049\n",
      "Epoch [35/100], Step [100/468], Loss: 0.0003\n",
      "Epoch [35/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [35/100], Step [300/468], Loss: 0.0027\n",
      "Epoch [35/100], Step [400/468], Loss: 0.0887\n",
      "Epoch [36/100], Step [100/468], Loss: 0.0011\n",
      "Epoch [36/100], Step [200/468], Loss: 0.0079\n",
      "Epoch [36/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [36/100], Step [400/468], Loss: 0.0398\n",
      "Epoch [37/100], Step [100/468], Loss: 0.0008\n",
      "Epoch [37/100], Step [200/468], Loss: 0.0078\n",
      "Epoch [37/100], Step [300/468], Loss: 0.0484\n",
      "Epoch [37/100], Step [400/468], Loss: 0.0005\n",
      "Epoch [38/100], Step [100/468], Loss: 0.0124\n",
      "Epoch [38/100], Step [200/468], Loss: 0.0161\n",
      "Epoch [38/100], Step [300/468], Loss: 0.1099\n",
      "Epoch [38/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [39/100], Step [100/468], Loss: 0.0112\n",
      "Epoch [39/100], Step [200/468], Loss: 0.0040\n",
      "Epoch [39/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [39/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [40/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [40/100], Step [200/468], Loss: 0.0080\n",
      "Epoch [40/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [40/100], Step [400/468], Loss: 0.0798\n",
      "Epoch [41/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [41/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [41/100], Step [300/468], Loss: 0.0017\n",
      "Epoch [41/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [42/100], Step [100/468], Loss: 0.0604\n",
      "Epoch [42/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [42/100], Step [300/468], Loss: 0.0245\n",
      "Epoch [42/100], Step [400/468], Loss: 0.0206\n",
      "Epoch [43/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [43/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [43/100], Step [300/468], Loss: 0.0211\n",
      "Epoch [43/100], Step [400/468], Loss: 0.0006\n",
      "Epoch [44/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [44/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [44/100], Step [300/468], Loss: 0.0009\n",
      "Epoch [44/100], Step [400/468], Loss: 0.0021\n",
      "Epoch [45/100], Step [100/468], Loss: 0.0380\n",
      "Epoch [45/100], Step [200/468], Loss: 0.0009\n",
      "Epoch [45/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [45/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [46/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [46/100], Step [200/468], Loss: 0.0010\n",
      "Epoch [46/100], Step [300/468], Loss: 0.0145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Step [400/468], Loss: 0.0039\n",
      "Epoch [47/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [47/100], Step [200/468], Loss: 0.0209\n",
      "Epoch [47/100], Step [300/468], Loss: 0.0015\n",
      "Epoch [47/100], Step [400/468], Loss: 0.0124\n",
      "Epoch [48/100], Step [100/468], Loss: 0.0481\n",
      "Epoch [48/100], Step [200/468], Loss: 0.0099\n",
      "Epoch [48/100], Step [300/468], Loss: 0.0101\n",
      "Epoch [48/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [49/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [49/100], Step [200/468], Loss: 0.1303\n",
      "Epoch [49/100], Step [300/468], Loss: 0.0249\n",
      "Epoch [49/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [50/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [50/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [51/100], Step [300/468], Loss: 0.0072\n",
      "Epoch [51/100], Step [400/468], Loss: 0.0004\n",
      "Epoch [52/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [52/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [52/100], Step [300/468], Loss: 0.0204\n",
      "Epoch [52/100], Step [400/468], Loss: 0.0874\n",
      "Epoch [53/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [53/100], Step [200/468], Loss: 0.0380\n",
      "Epoch [53/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [53/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [54/100], Step [100/468], Loss: 0.0009\n",
      "Epoch [54/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [54/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [54/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [55/100], Step [100/468], Loss: 0.0097\n",
      "Epoch [55/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [55/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [55/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [56/100], Step [100/468], Loss: 0.0004\n",
      "Epoch [56/100], Step [200/468], Loss: 0.0120\n",
      "Epoch [56/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [56/100], Step [400/468], Loss: 0.2109\n",
      "Epoch [57/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [57/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [57/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [57/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [58/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [58/100], Step [200/468], Loss: 0.0004\n",
      "Epoch [58/100], Step [300/468], Loss: 0.1281\n",
      "Epoch [58/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [59/100], Step [100/468], Loss: 0.0275\n",
      "Epoch [59/100], Step [200/468], Loss: 0.1803\n",
      "Epoch [59/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [59/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [60/100], Step [100/468], Loss: 0.0011\n",
      "Epoch [60/100], Step [200/468], Loss: 0.0015\n",
      "Epoch [60/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [60/100], Step [400/468], Loss: 0.0792\n",
      "Epoch [61/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [61/100], Step [200/468], Loss: 0.0677\n",
      "Epoch [61/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [61/100], Step [400/468], Loss: 0.0503\n",
      "Epoch [62/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [62/100], Step [200/468], Loss: 0.0048\n",
      "Epoch [62/100], Step [300/468], Loss: 0.0034\n",
      "Epoch [62/100], Step [400/468], Loss: 0.0005\n",
      "Epoch [63/100], Step [100/468], Loss: 0.0014\n",
      "Epoch [63/100], Step [200/468], Loss: 0.0017\n",
      "Epoch [63/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [63/100], Step [400/468], Loss: 0.0071\n",
      "Epoch [64/100], Step [100/468], Loss: 0.0010\n",
      "Epoch [64/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [64/100], Step [300/468], Loss: 0.0009\n",
      "Epoch [64/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [65/100], Step [100/468], Loss: 0.0039\n",
      "Epoch [65/100], Step [200/468], Loss: 0.0365\n",
      "Epoch [65/100], Step [300/468], Loss: 0.0111\n",
      "Epoch [65/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [66/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [66/100], Step [200/468], Loss: 0.0065\n",
      "Epoch [66/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [66/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [67/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [67/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [67/100], Step [300/468], Loss: 0.0837\n",
      "Epoch [67/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [68/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [68/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [100/468], Loss: 0.0250\n",
      "Epoch [69/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [69/100], Step [400/468], Loss: 0.0316\n",
      "Epoch [70/100], Step [100/468], Loss: 0.0048\n",
      "Epoch [70/100], Step [200/468], Loss: 0.0090\n",
      "Epoch [70/100], Step [300/468], Loss: 0.0517\n",
      "Epoch [70/100], Step [400/468], Loss: 0.0179\n",
      "Epoch [71/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [71/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [71/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [71/100], Step [400/468], Loss: 0.0387\n",
      "Epoch [72/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [72/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [72/100], Step [400/468], Loss: 0.0008\n",
      "Epoch [73/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [73/100], Step [200/468], Loss: 0.0295\n",
      "Epoch [73/100], Step [300/468], Loss: 0.0012\n",
      "Epoch [73/100], Step [400/468], Loss: 0.0003\n",
      "Epoch [74/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [74/100], Step [200/468], Loss: 0.0114\n",
      "Epoch [74/100], Step [300/468], Loss: 0.0361\n",
      "Epoch [74/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [75/100], Step [100/468], Loss: 0.0012\n",
      "Epoch [75/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [75/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [75/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [76/100], Step [400/468], Loss: 0.0077\n",
      "Epoch [77/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [77/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [77/100], Step [300/468], Loss: 0.0358\n",
      "Epoch [77/100], Step [400/468], Loss: 0.0319\n",
      "Epoch [78/100], Step [100/468], Loss: 0.0156\n",
      "Epoch [78/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [78/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [100/468], Loss: 0.0634\n",
      "Epoch [79/100], Step [200/468], Loss: 0.0434\n",
      "Epoch [79/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [79/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [80/100], Step [100/468], Loss: 0.0064\n",
      "Epoch [80/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [80/100], Step [300/468], Loss: 0.0989\n",
      "Epoch [80/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [81/100], Step [100/468], Loss: 0.0346\n",
      "Epoch [81/100], Step [200/468], Loss: 0.0050\n",
      "Epoch [81/100], Step [300/468], Loss: 0.0108\n",
      "Epoch [81/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [100/468], Loss: 0.0024\n",
      "Epoch [82/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [82/100], Step [300/468], Loss: 0.0022\n",
      "Epoch [82/100], Step [400/468], Loss: 0.1194\n",
      "Epoch [83/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [83/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [83/100], Step [300/468], Loss: 0.0046\n",
      "Epoch [83/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [100/468], Loss: 0.0233\n",
      "Epoch [84/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [84/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [85/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [85/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [86/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [86/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [86/100], Step [300/468], Loss: 0.1202\n",
      "Epoch [86/100], Step [400/468], Loss: 0.0025\n",
      "Epoch [87/100], Step [100/468], Loss: 0.0001\n",
      "Epoch [87/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [87/100], Step [300/468], Loss: 0.0003\n",
      "Epoch [87/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [88/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [88/100], Step [200/468], Loss: 0.0449\n",
      "Epoch [88/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [88/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [89/100], Step [100/468], Loss: 0.0002\n",
      "Epoch [89/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [89/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [89/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [90/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [91/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [91/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [91/100], Step [300/468], Loss: 0.0014\n",
      "Epoch [91/100], Step [400/468], Loss: 0.0068\n",
      "Epoch [92/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [92/100], Step [200/468], Loss: 0.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [300/468], Loss: 0.0002\n",
      "Epoch [92/100], Step [400/468], Loss: 0.0044\n",
      "Epoch [93/100], Step [100/468], Loss: 0.0004\n",
      "Epoch [93/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [93/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [93/100], Step [400/468], Loss: 0.0001\n",
      "Epoch [94/100], Step [100/468], Loss: 0.0117\n",
      "Epoch [94/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [94/100], Step [300/468], Loss: 0.0806\n",
      "Epoch [94/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [95/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [96/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [96/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [96/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [96/100], Step [400/468], Loss: 0.0123\n",
      "Epoch [97/100], Step [100/468], Loss: 0.0217\n",
      "Epoch [97/100], Step [200/468], Loss: 0.0001\n",
      "Epoch [97/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [97/100], Step [400/468], Loss: 0.0035\n",
      "Epoch [98/100], Step [100/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [98/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [98/100], Step [400/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [100/468], Loss: 0.0028\n",
      "Epoch [99/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [300/468], Loss: 0.0000\n",
      "Epoch [99/100], Step [400/468], Loss: 0.0002\n",
      "Epoch [100/100], Step [100/468], Loss: 0.0082\n",
      "Epoch [100/100], Step [200/468], Loss: 0.0000\n",
      "Epoch [100/100], Step [300/468], Loss: 0.0001\n",
      "Epoch [100/100], Step [400/468], Loss: 0.0001\n",
      "Accuracy of the network on test images:  98.14\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "#         self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "#         out = F.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "nhid = 800\n",
    "net = Net(784, nhid, 10)\n",
    "net.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "##############################################################################\n",
    "num_epochs = 100\n",
    "batch_size=128\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n",
    "                                            num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):   # Load a batch of images with its (index, data, class)\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images.cuda())                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels.cuda())                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0: # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, \n",
    "                   num_epochs, \n",
    "                   i+1, \n",
    "                   len(trainset)//batch_size, \n",
    "                   loss.data.item()))\n",
    "            \n",
    "#######################################\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in valloader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images.cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels.cuda()).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on test images: ', 100 * correct.item() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.concatenate((net.fc1.weight.cpu().data.reshape(nhid*784), net.fc2.weight.cpu().data.reshape(nhid**2), net.fc3.weight.cpu().data.reshape(nhid*10)))\n",
    "pickle.dump(weights, open('results/ffnnweights.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988000\n",
      "198800\n",
      "198800\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "bbb_weights = pickle.load(open('results/BBBweights.pkl', 'rb'))\n",
    "dropout_weights = pickle.load(open('results/dropoutweights.pkl', 'rb'))\n",
    "ffnn_weights = pickle.load(open('results/ffnnweights.pkl', 'rb'))\n",
    "\n",
    "# pickle.dump(bbb_weights, open('BBBweights256.pkl', 'rb'))\n",
    "# pickle.dump(dropout_weights, open('dropoutweights256.pkl', 'rb'))\n",
    "# pickle.dump(ffnn_weights, open('SGDweights256.pkl', 'rb'))\n",
    "\n",
    "\n",
    "print(len(bbb_weights))\n",
    "print(len(dropout_weights))\n",
    "print(len(ffnn_weights))\n",
    "\n",
    "# dropout_weights = net.get_weight_samples()\n",
    "\n",
    "plt.hist(bbb_weights, density=True, bins=np.linspace(-0.1, 0.1, 50), alpha=0.5, label='Bayes by Backprop')\n",
    "plt.hist(dropout_weights, density=True, bins=np.linspace(-0.1, 0.1, 50), alpha=0.5, label='Vanilla SGD')\n",
    "plt.hist(ffnn_weights, density=True, bins=np.linspace(-0.1, 0.1, 50), alpha=0.5, label='MC Dropout')\n",
    "plt.legend()\n",
    "plt.xlim((-0.1, 0.1))\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Sampled Weights')\n",
    "plt.title('Distribution of Weights')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(bbb_weights, open('BBBweights1200.pkl', 'wb'))\n",
    "# pickle.dump(dropout_weights, open('dropoutweights1200.pkl', 'wb'))\n",
    "# pickle.dump(ffnn_weights, open('SGDweights1200.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.5, 0.75, 0.95, 0.98]\n",
      "-inf\n",
      "params: 199210\n",
      "Delete proportion: 0.000000\n",
      "\u001b[34m    Jtest = 0.107873, err = 0.025000\n",
      "\u001b[0m\n",
      "-10.731100 dB\n",
      "params: 99605\n",
      "Delete proportion: 0.500000\n",
      "\u001b[34m    Jtest = 0.099280, err = 0.024700\n",
      "\u001b[0m\n",
      "-8.541501 dB\n",
      "params: 49803\n",
      "Delete proportion: 0.750000\n",
      "\u001b[34m    Jtest = 0.096376, err = 0.024700\n",
      "\u001b[0m\n",
      "-2.043322 dB\n",
      "params: 9961\n",
      "Delete proportion: 0.950000\n",
      "\u001b[34m    Jtest = 0.103565, err = 0.028500\n",
      "\u001b[0m\n",
      "0.739247 dB\n",
      "params: 3985\n",
      "Delete proportion: 0.980000\n",
      "\u001b[34m    Jtest = 0.188387, err = 0.050000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f04088cfc50>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create pruning graphs\n",
    "net = torch.load('bbb.net')\n",
    "SNR_vector = net.get_weight_SNR()\n",
    "\n",
    "delete_proportions = [0, 0.5, 0.75, 0.95, 0.98]\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "# keep_proportions = 1 - delete_proportions\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "Nsamples_predict = 100\n",
    "# Nsamples_KLD = 20\n",
    "use_cuda = torch.cuda.is_available()\n",
    "net.set_mode_train(False)\n",
    "\n",
    "if use_cuda:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "else:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=4)\n",
    "err_vec = np.zeros(len(delete_proportions))\n",
    "\n",
    "print(delete_proportions)\n",
    "\n",
    "for idx, p in enumerate(delete_proportions):\n",
    "    if p > 0:\n",
    "        min_snr = np.percentile(SNR_vector, p*100)\n",
    "        print('%f dB' % (10*np.log10(min_snr)))\n",
    "        og_state_dict, n_unmask = net.mask_model(Nsamples=0, thresh=min_snr)\n",
    "#         print(net.model.state_dict()) # for debug purposes\n",
    "        print('params: %d' % (n_unmask))\n",
    "    else:\n",
    "        print('-inf')\n",
    "        print('params: %d' % (net.get_nb_parameters()/2))\n",
    "    \n",
    "    test_cost = 0  # Note that these are per sample\n",
    "    test_err = 0\n",
    "    nb_samples = 0\n",
    "    for j, (x, y) in enumerate(valloader):\n",
    "        cost, err, probs = net.sample_eval(x, y, Nsamples_predict, logits=False) # , logits=True\n",
    "\n",
    "        test_cost += cost\n",
    "        test_err += err.cpu().numpy()\n",
    "#         test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    test_cost /= nb_samples\n",
    "    test_err /= nb_samples\n",
    "\n",
    "    print('Delete proportion: %f' % (p))\n",
    "    cprint('b', '    Jtest = %1.6f, err = %1.6f\\n' % (test_cost, test_err))\n",
    "    err_vec[idx] = test_err\n",
    "    \n",
    "    if p > 0:\n",
    "        net.model.load_state_dict(og_state_dict)\n",
    "    \n",
    "    \n",
    "plt.figure(dpi=100)\n",
    "plt.plot(delete_proportions, err_vec, 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
