{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import time\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "import matplotlib\n",
    "from Bayes_By_Backprop.model import *\n",
    "from Bayes_By_Backprop_Local_Reparametrization.model import *\n",
    "import time\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (8124, 118)\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "data = genfromtxt('onehot.csv', delimiter=',', skip_header=True)[:,1:]\n",
    "print('data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression BNN\n",
    "%matplotlib inline\n",
    "import GPy\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def log_gaussian_loss(output, target, sigma, no_dim):\n",
    "    exponent = -0.5*(target - output)**2/sigma**2\n",
    "    log_coeff = -no_dim*torch.log(sigma)\n",
    "    \n",
    "    return - (log_coeff + exponent).sum()\n",
    "\n",
    "\n",
    "def get_kl_divergence(weights, prior, varpost):\n",
    "    prior_loglik = prior.loglik(weights)\n",
    "    \n",
    "    varpost_loglik = varpost.loglik(weights)\n",
    "    varpost_lik = varpost_loglik.exp()\n",
    "    \n",
    "    return (varpost_lik*(varpost_loglik - prior_loglik)).sum()\n",
    "\n",
    "\n",
    "class gaussian:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def loglik(self, weights):\n",
    "        exponent = -0.5*(weights - self.mu)**2/self.sigma**2\n",
    "        log_coeff = -0.5*(np.log(2*np.pi) + 2*np.log(self.sigma))\n",
    "        \n",
    "        return (exponent + log_coeff).sum()\n",
    "    \n",
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "        \n",
    "        scale = (2/self.input_dim)**0.5\n",
    "        rho_init = np.log(np.exp((2/self.input_dim)**0.5) - 1)\n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.05, 0.05))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-2, -1))\n",
    "        \n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.05, 0.05))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-2, -1))\n",
    "        \n",
    "    def forward(self, x, sample = True):\n",
    "        \n",
    "        if sample:\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "            bias_epsilons =  Variable(self.bias_mus.data.new(self.bias_mus.size()).normal_())\n",
    "            \n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            bias_stds = torch.log(1 + torch.exp(self.bias_rhos))\n",
    "            \n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            bias_sample = self.bias_mus + bias_epsilons*bias_stds\n",
    "            \n",
    "            output = torch.mm(x, weight_sample) + bias_sample\n",
    "            \n",
    "            # computing the KL loss term\n",
    "            prior_cov, varpost_cov = self.prior.sigma**2, weight_stds**2\n",
    "            KL_loss = 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*weight_stds.numel()\n",
    "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "            KL_loss = KL_loss + 0.5*((self.weight_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "            prior_cov, varpost_cov = self.prior.sigma**2, bias_stds**2\n",
    "            KL_loss = KL_loss + 0.5*(torch.log(prior_cov/varpost_cov)).sum() - 0.5*bias_stds.numel()\n",
    "            KL_loss = KL_loss + 0.5*(varpost_cov/prior_cov).sum()\n",
    "            KL_loss = KL_loss + 0.5*((self.bias_mus - self.prior.mu)**2/prior_cov).sum()\n",
    "            \n",
    "            return output, KL_loss\n",
    "        \n",
    "        else:\n",
    "            output = torch.mm(x, self.weight_mus) + self.bias_mus\n",
    "            return output, KL_loss\n",
    "        \n",
    "    def sample_layer(self, no_samples):\n",
    "        all_samples = []\n",
    "        for i in range(no_samples):\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "            \n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            \n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons*weight_stds\n",
    "            \n",
    "            all_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n",
    "            \n",
    "        return all_samples\n",
    "    \n",
    "class BBP_Homoscedastic_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, no_units, init_log_noise):\n",
    "        super(BBP_Homoscedastic_Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, no_units, gaussian(0, 1))\n",
    "        self.layer2 = BayesLinear_Normalq(no_units, output_dim, gaussian(0, 1))\n",
    "        \n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.ReLU(inplace = True)\n",
    "        self.log_noise = nn.Parameter(torch.cuda.FloatTensor([init_log_noise]))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        x, KL_loss = self.layer1(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x, KL_loss = self.layer2(x)\n",
    "        KL_loss_total = KL_loss_total + KL_loss\n",
    "        \n",
    "        return x, KL_loss_total\n",
    "    \n",
    "\n",
    "class BBP_Homoscedastic_Model_Wrapper:\n",
    "    def __init__(self, input_dim, output_dim, no_units, learn_rate, batch_size, no_batches, init_log_noise):\n",
    "        \n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "        \n",
    "        self.network = BBP_Homoscedastic_Model(input_dim = input_dim, output_dim = output_dim,\n",
    "                                               no_units = no_units, init_log_noise = init_log_noise)\n",
    "        self.network.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.network.parameters(), lr = self.learn_rate) \n",
    "        # self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "    \n",
    "    def fit(self, x, y, no_samples):\n",
    "        x, y = to_variable(var=(x, y), cuda=True)\n",
    "        \n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "        fit_loss_total = 0\n",
    "        \n",
    "        for i in range(no_samples):\n",
    "            output, KL_loss_total = self.network(x)\n",
    "\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss_total = fit_loss_total + self.loss_func(output, y, self.network.log_noise.exp(), self.network.output_dim)\n",
    "        \n",
    "        KL_loss_total = KL_loss_total/self.no_batches\n",
    "        total_loss = (fit_loss_total + KL_loss_total)/(no_samples*x.shape[0])\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return fit_loss_total/no_samples, KL_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 regret: 310.0 fit loss: 430206.71875 kl loss: 13640.7578125\n",
      "iteration: 100 regret: 165.0 fit loss: 17438.357421875 kl loss: 13640.7685546875\n",
      "iteration: 200 regret: 705.0 fit loss: 163864.890625 kl loss: 13640.775390625\n",
      "iteration: 300 regret: 100.0 fit loss: 11838.150390625 kl loss: 13640.779296875\n",
      "iteration: 400 regret: 150.0 fit loss: 9548.328125 kl loss: 13640.783203125\n",
      "iteration: 500 regret: 160.0 fit loss: 5436.76220703125 kl loss: 13640.78515625\n",
      "iteration: 600 regret: 205.0 fit loss: 15260.216796875 kl loss: 13640.78515625\n",
      "iteration: 700 regret: 225.0 fit loss: 11463.552734375 kl loss: 13640.7861328125\n",
      "iteration: 800 regret: 530.0 fit loss: 43136.38671875 kl loss: 13640.787109375\n",
      "iteration: 900 regret: 580.0 fit loss: 38995.3125 kl loss: 13640.7880859375\n",
      "iteration: 1000 regret: 305.0 fit loss: 16213.818359375 kl loss: 13640.7890625\n",
      "iteration: 1100 regret: 180.0 fit loss: 14373.173828125 kl loss: 13640.7900390625\n",
      "iteration: 1200 regret: 315.0 fit loss: 22454.93359375 kl loss: 13640.791015625\n",
      "iteration: 1300 regret: 435.0 fit loss: 22539.267578125 kl loss: 13640.791015625\n",
      "iteration: 1400 regret: 415.0 fit loss: 24806.43359375 kl loss: 13640.7919921875\n",
      "iteration: 1500 regret: 190.0 fit loss: 12967.2890625 kl loss: 13640.79296875\n",
      "iteration: 1600 regret: 310.0 fit loss: 19603.373046875 kl loss: 13640.79296875\n",
      "iteration: 1700 regret: 520.0 fit loss: 27855.671875 kl loss: 13640.7939453125\n",
      "iteration: 1800 regret: 420.0 fit loss: 24661.10546875 kl loss: 13640.7939453125\n",
      "iteration: 1900 regret: 220.0 fit loss: 12282.3642578125 kl loss: 13640.794921875\n",
      "iteration: 2000 regret: 80.0 fit loss: 7842.64306640625 kl loss: 13640.794921875\n",
      "iteration: 2100 regret: 280.0 fit loss: 12912.7197265625 kl loss: 13640.7958984375\n",
      "iteration: 2200 regret: 305.0 fit loss: 15869.29296875 kl loss: 13640.7958984375\n",
      "iteration: 2300 regret: 320.0 fit loss: 17628.845703125 kl loss: 13640.796875\n",
      "iteration: 2400 regret: 420.0 fit loss: 17281.138671875 kl loss: 13640.796875\n",
      "iteration: 2500 regret: 635.0 fit loss: 23832.025390625 kl loss: 13640.796875\n",
      "iteration: 2600 regret: 130.0 fit loss: 11028.390625 kl loss: 13640.7978515625\n",
      "iteration: 2700 regret: 150.0 fit loss: 8039.8193359375 kl loss: 13640.7978515625\n",
      "iteration: 2800 regret: 140.0 fit loss: 8157.7685546875 kl loss: 13640.798828125\n",
      "iteration: 2900 regret: 115.0 fit loss: 8349.728515625 kl loss: 13640.798828125\n",
      "iteration: 3000 regret: 180.0 fit loss: 10602.689453125 kl loss: 13640.798828125\n",
      "iteration: 3100 regret: 135.0 fit loss: 9997.96484375 kl loss: 13640.798828125\n",
      "iteration: 3200 regret: 290.0 fit loss: 11965.5224609375 kl loss: 13640.7998046875\n",
      "iteration: 3300 regret: 320.0 fit loss: 14226.328125 kl loss: 13640.7998046875\n",
      "iteration: 3400 regret: 105.0 fit loss: 8467.087890625 kl loss: 13640.7998046875\n",
      "iteration: 3500 regret: 235.0 fit loss: 11256.392578125 kl loss: 13640.80078125\n",
      "iteration: 3600 regret: 235.0 fit loss: 10559.0810546875 kl loss: 13640.80078125\n",
      "iteration: 3700 regret: 260.0 fit loss: 11796.7490234375 kl loss: 13640.80078125\n",
      "iteration: 3800 regret: 270.0 fit loss: 14691.27734375 kl loss: 13640.80078125\n",
      "iteration: 3900 regret: 190.0 fit loss: 8620.8056640625 kl loss: 13640.8017578125\n",
      "iteration: 4000 regret: 145.0 fit loss: 8631.626953125 kl loss: 13640.8017578125\n",
      "iteration: 4100 regret: 115.0 fit loss: 9409.583984375 kl loss: 13640.8017578125\n",
      "iteration: 4200 regret: 140.0 fit loss: 10777.4580078125 kl loss: 13640.8017578125\n",
      "iteration: 4300 regret: 450.0 fit loss: 17835.177734375 kl loss: 13640.802734375\n",
      "iteration: 4400 regret: 100.0 fit loss: 8800.83984375 kl loss: 13640.802734375\n",
      "iteration: 4500 regret: 145.0 fit loss: 8786.6923828125 kl loss: 13640.802734375\n",
      "iteration: 4600 regret: 150.0 fit loss: 8808.5732421875 kl loss: 13640.802734375\n",
      "iteration: 4700 regret: 145.0 fit loss: 10075.455078125 kl loss: 13640.802734375\n",
      "iteration: 4800 regret: 40.0 fit loss: 9067.1904296875 kl loss: 13640.8037109375\n",
      "iteration: 4900 regret: 465.0 fit loss: 17200.76171875 kl loss: 13640.8037109375\n",
      "iteration: 5000 regret: 235.0 fit loss: 11634.810546875 kl loss: 13640.8037109375\n",
      "iteration: 5100 regret: 420.0 fit loss: 13583.517578125 kl loss: 13640.8037109375\n",
      "iteration: 5200 regret: 650.0 fit loss: 19416.25390625 kl loss: 13640.8037109375\n",
      "iteration: 5300 regret: 275.0 fit loss: 14921.26171875 kl loss: 13640.8037109375\n",
      "iteration: 5400 regret: 100.0 fit loss: 11976.6572265625 kl loss: 13640.8046875\n",
      "iteration: 5500 regret: 140.0 fit loss: 9593.08984375 kl loss: 13640.8046875\n",
      "iteration: 5600 regret: 285.0 fit loss: 11990.09375 kl loss: 13640.8046875\n",
      "iteration: 5700 regret: 170.0 fit loss: 10095.9111328125 kl loss: 13640.8046875\n",
      "iteration: 5800 regret: 245.0 fit loss: 10582.5048828125 kl loss: 13640.8046875\n",
      "iteration: 5900 regret: 380.0 fit loss: 15042.0244140625 kl loss: 13640.8046875\n",
      "iteration: 6000 regret: 110.0 fit loss: 10746.779296875 kl loss: 13640.8056640625\n",
      "iteration: 6100 regret: 60.0 fit loss: 9914.59765625 kl loss: 13640.8056640625\n",
      "iteration: 6200 regret: 455.0 fit loss: 16259.6064453125 kl loss: 13640.8056640625\n",
      "iteration: 6300 regret: 130.0 fit loss: 9256.96875 kl loss: 13640.8056640625\n",
      "iteration: 6400 regret: 290.0 fit loss: 12887.736328125 kl loss: 13640.8056640625\n",
      "iteration: 6500 regret: 335.0 fit loss: 12770.380859375 kl loss: 13640.8056640625\n",
      "iteration: 6600 regret: 510.0 fit loss: 15070.92578125 kl loss: 13640.8056640625\n",
      "iteration: 6700 regret: 370.0 fit loss: 13787.7314453125 kl loss: 13640.806640625\n",
      "iteration: 6800 regret: 370.0 fit loss: 13261.943359375 kl loss: 13640.806640625\n",
      "iteration: 6900 regret: 310.0 fit loss: 14816.640625 kl loss: 13640.806640625\n",
      "iteration: 7000 regret: 270.0 fit loss: 14359.28515625 kl loss: 13640.806640625\n",
      "iteration: 7100 regret: 85.0 fit loss: 9514.626953125 kl loss: 13640.806640625\n",
      "iteration: 7200 regret: 830.0 fit loss: 19442.9921875 kl loss: 13640.806640625\n",
      "iteration: 7300 regret: 180.0 fit loss: 10322.025390625 kl loss: 13640.806640625\n",
      "iteration: 7400 regret: 385.0 fit loss: 14999.130859375 kl loss: 13640.8076171875\n",
      "iteration: 7500 regret: 565.0 fit loss: 16953.67578125 kl loss: 13640.8076171875\n",
      "iteration: 7600 regret: 260.0 fit loss: 11045.4345703125 kl loss: 13640.8076171875\n",
      "iteration: 7700 regret: 300.0 fit loss: 11035.060546875 kl loss: 13640.8076171875\n",
      "iteration: 7800 regret: 395.0 fit loss: 14016.7841796875 kl loss: 13640.8076171875\n",
      "iteration: 7900 regret: 605.0 fit loss: 16196.416015625 kl loss: 13640.8076171875\n",
      "iteration: 8000 regret: 495.0 fit loss: 16011.0126953125 kl loss: 13640.8076171875\n",
      "iteration: 8100 regret: 200.0 fit loss: 10390.1142578125 kl loss: 13640.8076171875\n",
      "iteration: 8200 regret: 30.0 fit loss: 10174.5263671875 kl loss: 13640.80859375\n",
      "iteration: 8300 regret: 495.0 fit loss: 14151.1298828125 kl loss: 13640.80859375\n",
      "iteration: 8400 regret: 470.0 fit loss: 14615.05078125 kl loss: 13640.80859375\n",
      "iteration: 8500 regret: 585.0 fit loss: 16850.66796875 kl loss: 13640.80859375\n",
      "iteration: 8600 regret: 630.0 fit loss: 16016.361328125 kl loss: 13640.80859375\n",
      "iteration: 8700 regret: 475.0 fit loss: 14553.892578125 kl loss: 13640.80859375\n",
      "iteration: 8800 regret: 215.0 fit loss: 11174.1796875 kl loss: 13640.80859375\n",
      "iteration: 8900 regret: 455.0 fit loss: 13608.7900390625 kl loss: 13640.80859375\n",
      "iteration: 9000 regret: 635.0 fit loss: 16698.701171875 kl loss: 13640.80859375\n",
      "iteration: 9100 regret: 230.0 fit loss: 13599.7529296875 kl loss: 13640.8095703125\n",
      "iteration: 9200 regret: 130.0 fit loss: 9701.7216796875 kl loss: 13640.8095703125\n",
      "iteration: 9300 regret: 145.0 fit loss: 9705.0556640625 kl loss: 13640.8095703125\n",
      "iteration: 9400 regret: 160.0 fit loss: 9711.5126953125 kl loss: 13640.8095703125\n",
      "iteration: 9500 regret: 240.0 fit loss: 12741.6298828125 kl loss: 13640.8095703125\n",
      "iteration: 9600 regret: 470.0 fit loss: 15154.365234375 kl loss: 13640.8095703125\n",
      "iteration: 9700 regret: 375.0 fit loss: 13752.0546875 kl loss: 13640.8095703125\n",
      "iteration: 9800 regret: 290.0 fit loss: 13875.3212890625 kl loss: 13640.8095703125\n",
      "iteration: 9900 regret: 450.0 fit loss: 14794.85546875 kl loss: 13640.8095703125\n",
      "time elapsed: 1488.4792234897614\n"
     ]
    }
   ],
   "source": [
    "# train BNN\n",
    "t0 = time.time()\n",
    "\n",
    "bnet = BBP_Homoscedastic_Model_Wrapper(input_dim = 119, output_dim = 1, no_units = 100, learn_rate = 1e-6,\n",
    "                                      batch_size = 64, no_batches = 1, init_log_noise = 0)\n",
    "\n",
    "num_episodes=10000\n",
    "batch_size=64\n",
    "regret = []\n",
    "eat = np.array([[1,0] for _ in range(batch_size)])\n",
    "ignore = np.array([[0,1] for _ in range(batch_size)])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # get 64 mushrooms\n",
    "    idx = np.random.randint(len(data), size=batch_size)\n",
    "    data_train = data[idx,:]\n",
    "    X, poisonous = data_train[:,1:], data_train[:,0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get scores for eating and ignoring\n",
    "    eat_array = np.append(X, eat, axis=1)\n",
    "    eat_scores, loss = bnet.network.forward(torch.tensor(eat_array).float().cuda())\n",
    "    ignore_array = np.append(X, ignore, axis=1)\n",
    "    ignore_scores, loss = bnet.network.forward(torch.tensor(ignore_array).float().cuda())\n",
    "    \n",
    "    x_train, y_train = np.zeros(eat_array.shape), np.zeros(poisonous.shape)\n",
    "    \n",
    "    # get training vectors\n",
    "    diffs = (eat_scores.cpu().detach().numpy() - ignore_scores.cpu().detach().numpy()).reshape((batch_size,))\n",
    "    x_train[diffs>0] = eat_array[diffs>0] # ate it\n",
    "    x_train[diffs<=0] = ignore_array[diffs<=0] # ignored it\n",
    "    \n",
    "    # get training labels\n",
    "    vals = np.random.normal(0, 1, batch_size)\n",
    "    y_train[vals>0] = -35\n",
    "    y_train[vals<=0] = 5\n",
    "    y_train[poisonous==0] = 5 # ate it (safe)\n",
    "    y_train[diffs <= 0] = 0 # if you ignored it\n",
    "\n",
    "    \n",
    "    # get regret for this episode\n",
    "    oracle_score = 5 * sum(poisonous==1) # optimal strategy eats all good and ignores all bad\n",
    "    agent_score = sum(y_train)\n",
    "    regret.append(oracle_score - agent_score)\n",
    "    \n",
    "    # backprop\n",
    "    fit_loss, KL_loss = bnet.fit(x_train, y_train, no_samples = batch_size)\n",
    "\n",
    "    if not i % 100:\n",
    "        print('iteration:', i, 'regret:', oracle_score - agent_score, 'fit loss:', fit_loss.item(), 'kl loss:', KL_loss.item())\n",
    "        \n",
    "print('time elapsed:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# cum_regrets = list(regret)\n",
    "# for i in range(1, len(cum_regrets)): cum_regrets[i] += cum_regrets[i-1]\n",
    "\n",
    "# bnn_cum_regrets = cum_regrets\n",
    "# pickle.dump(bnn_cum_regrets, open('results/bnn_regret.pkl', 'wb'))\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.plot(bnn_cum_regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 100 regret: 235.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /homes/awl27/testenv/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning:To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200 regret: 195.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 300 regret: 210.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 400 regret: 205.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 500 regret: 250.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 600 regret: 180.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 700 regret: 220.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 800 regret: 190.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 900 regret: 215.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1000 regret: 145.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1100 regret: 185.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1200 regret: 210.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1300 regret: 175.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1400 regret: 295.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1500 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1600 regret: 145.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1700 regret: 110.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1800 regret: 150.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1900 regret: 205.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2000 regret: 185.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2100 regret: 295.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2200 regret: 135.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2300 regret: 140.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2400 regret: 175.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2500 regret: 190.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2600 regret: 225.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2700 regret: 130.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2800 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2900 regret: 95.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3000 regret: 115.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3100 regret: 210.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3200 regret: 220.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3300 regret: 170.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3400 regret: 315.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3500 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3600 regret: 230.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3700 regret: 110.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3800 regret: 250.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3900 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4000 regret: 260.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4100 regret: 200.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4200 regret: 165.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4300 regret: 190.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4400 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4500 regret: 120.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4600 regret: 170.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4700 regret: 150.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4800 regret: 210.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4900 regret: 130.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5000 regret: 235.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5100 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5200 regret: 135.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5300 regret: 270.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5400 regret: 255.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5500 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5600 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5700 regret: 180.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5800 regret: 105.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5900 regret: 225.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6000 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6100 regret: 290.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6200 regret: 200.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6300 regret: 295.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6400 regret: 200.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6500 regret: 260.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6600 regret: 175.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6700 regret: 150.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6800 regret: 200.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6900 regret: 230.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7000 regret: 340.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7100 regret: 250.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7200 regret: 165.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7300 regret: 205.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7400 regret: 200.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7500 regret: 270.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7600 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7700 regret: 280.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7800 regret: 290.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7900 regret: 190.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8000 regret: 165.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8100 regret: 130.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8200 regret: 175.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8300 regret: 155.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8400 regret: 125.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8500 regret: 190.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8600 regret: 325.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8700 regret: 135.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8800 regret: 320.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8900 regret: 165.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9000 regret: 260.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9100 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9200 regret: 95.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9300 regret: 165.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9400 regret: 250.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9500 regret: 160.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9600 regret: 140.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9700 regret: 215.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9800 regret: 195.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9900 regret: 195.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "time elapsed: 12.115429401397705\n"
     ]
    }
   ],
   "source": [
    "# train greedy FNN\n",
    "np.random.seed(1)\n",
    "t0 = time.time()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "nhid = 100\n",
    "fnet = Net(119, nhid, 1)\n",
    "fnet.cuda()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)\n",
    "\n",
    "num_episodes=10000\n",
    "batch_size=64\n",
    "regret = []\n",
    "eat = np.array([[1,0] for _ in range(batch_size)])\n",
    "ignore = np.array([[0,1] for _ in range(batch_size)])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # get 64 mushrooms\n",
    "    idx = np.random.randint(len(data), size=batch_size)\n",
    "    data_train = data[idx,:]\n",
    "    X, poisonous = data_train[:,1:], data_train[:,0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get scores for eating and ignoring\n",
    "    eat_array = np.append(X, eat, axis=1)\n",
    "    eat_scores = fnet(torch.tensor(eat_array).float().cuda())\n",
    "    ignore_array = np.append(X, ignore, axis=1)\n",
    "    ignore_scores = fnet(torch.tensor(ignore_array).float().cuda())\n",
    "    \n",
    "    x_train, y_train = np.zeros(eat_array.shape), np.zeros(poisonous.shape)\n",
    "    \n",
    "    # get training vectors\n",
    "    diffs = (eat_scores.cpu().detach().numpy() - ignore_scores.cpu().detach().numpy()).reshape((batch_size,))\n",
    "    x_train[diffs>0] = eat_array[diffs>0] # ate it\n",
    "    x_train[diffs<=0] = ignore_array[diffs<=0] # ignored it\n",
    "    \n",
    "    # get training labels\n",
    "    vals = np.random.normal(0, 1, batch_size)\n",
    "    y_train[vals>0] = -35\n",
    "    y_train[vals<=0] = 5\n",
    "    y_train[poisonous==0] = 5 # ate it (safe)\n",
    "    y_train[diffs <= 0] = 0 # if you ignored it\n",
    "\n",
    "    \n",
    "    # get regret for this episode\n",
    "    oracle_score = 5 * sum(poisonous==1) # optimal strategy eats all good and ignores all bad\n",
    "    agent_score = sum(y_train)\n",
    "    regret.append(oracle_score - agent_score)\n",
    "    \n",
    "    # backprop\n",
    "    outputs = eat_scores\n",
    "    outputs[diffs<=0] = ignore_scores[diffs<=0]\n",
    "    outputs = outputs.reshape((batch_size,))\n",
    "    loss = criterion(torch.tensor(outputs).cuda(), torch.tensor(y_train).cuda())\n",
    "    loss = Variable(loss, requires_grad = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not i % 100:\n",
    "        print('iteration:', i, 'regret:', oracle_score - agent_score, 'fit loss:', fit_loss.item(), 'kl loss:', KL_loss.item())\n",
    "\n",
    "print('time elapsed:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# cum_regrets = list(regret)\n",
    "# for i in range(1, len(cum_regrets)): cum_regrets[i] += cum_regrets[i-1]\n",
    "\n",
    "# fnn_cum_regrets = cum_regrets\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.plot(fnn_cum_regrets[1:500], label='greedy')\n",
    "# plt.plot(bnn_cum_regrets[1:500], label='bnn')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 regret: 140.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /homes/awl27/testenv/lib/python3.5/site-packages/ipykernel_launcher.py:62: UserWarning:To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100 regret: 510.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 200 regret: 375.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 300 regret: 685.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 400 regret: 340.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 500 regret: 640.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 600 regret: 530.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 700 regret: 595.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 800 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 900 regret: 655.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1000 regret: 580.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1100 regret: 590.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1200 regret: 450.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1300 regret: 645.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1400 regret: 450.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1500 regret: 565.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1600 regret: 705.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1700 regret: 525.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1800 regret: 360.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1900 regret: 625.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2000 regret: 660.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2100 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2200 regret: 630.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2300 regret: 230.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2400 regret: 300.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2500 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2600 regret: 445.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2700 regret: 570.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2800 regret: 300.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2900 regret: 395.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3000 regret: 785.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3100 regret: 730.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3200 regret: 255.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3300 regret: 625.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3400 regret: 480.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3500 regret: 470.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3600 regret: 220.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3700 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3800 regret: 570.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3900 regret: 580.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4000 regret: 450.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4100 regret: 395.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4200 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4300 regret: 765.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4400 regret: 175.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4500 regret: 355.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4600 regret: 305.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4700 regret: 605.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4800 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4900 regret: 495.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5000 regret: 460.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5100 regret: 580.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5200 regret: 480.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5300 regret: 685.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5400 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5500 regret: 495.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5600 regret: 360.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5700 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5800 regret: 645.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5900 regret: 510.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6000 regret: 70.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6100 regret: 210.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6200 regret: 500.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6300 regret: 440.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6400 regret: 460.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6500 regret: 425.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6600 regret: 515.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6700 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6800 regret: 310.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6900 regret: 365.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7000 regret: 440.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7100 regret: 410.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7200 regret: 375.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7300 regret: 370.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7400 regret: 685.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7500 regret: 675.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7600 regret: 320.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7700 regret: 615.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7800 regret: 600.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7900 regret: 395.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8000 regret: 295.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8100 regret: 245.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8200 regret: 455.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8300 regret: 375.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8400 regret: 450.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8500 regret: 520.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8600 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8700 regret: 645.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8800 regret: 605.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8900 regret: 665.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9000 regret: 890.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9100 regret: 320.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9200 regret: 700.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9300 regret: 480.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9400 regret: 570.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9500 regret: 610.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9600 regret: 380.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9700 regret: 475.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9800 regret: 475.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9900 regret: 465.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "time elapsed: 12.377790212631226\n"
     ]
    }
   ],
   "source": [
    "# train 1% greedy FNN\n",
    "t0 = time.time()\n",
    "\n",
    "#initialize net\n",
    "fnet = Net(119, nhid, 1)\n",
    "fnet.cuda()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)\n",
    "\n",
    "epsilon = 0.01\n",
    "num_episodes=10000\n",
    "batch_size=64\n",
    "regret = []\n",
    "eat = np.array([[1,0] for _ in range(batch_size)])\n",
    "ignore = np.array([[0,1] for _ in range(batch_size)])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # get 64 mushrooms\n",
    "    idx = np.random.randint(len(data), size=batch_size)\n",
    "    data_train = data[idx,:]\n",
    "    X, poisonous = data_train[:,1:], data_train[:,0]\n",
    "    \n",
    "    # get scores for eating and ignoring\n",
    "    eat_array = np.append(X, eat, axis=1)\n",
    "    eat_scores = fnet(torch.tensor(eat_array).float().cuda())\n",
    "    ignore_array = np.append(X, ignore, axis=1)\n",
    "    ignore_scores = fnet(torch.tensor(ignore_array).float().cuda())\n",
    "    \n",
    "    x_train, y_train = np.zeros(eat_array.shape), np.zeros(poisonous.shape)\n",
    "    \n",
    "    # get training vectors\n",
    "    diffs = (eat_scores.cpu().detach().numpy() - ignore_scores.cpu().detach().numpy()).reshape((batch_size,))\n",
    "    \n",
    "    # epsilon greedy behavior change to score differences\n",
    "    eps = np.random.uniform(0, 1, batch_size) # generate uniform vector\n",
    "    random_actions = np.random.normal(0, 1, batch_size)\n",
    "    random_actions[random_actions>0] = 1\n",
    "    random_actions[random_actions<=0] = -1\n",
    "    diffs[eps > 1 - epsilon] = random_actions[eps > 1 - epsilon]\n",
    "    \n",
    "    x_train[diffs>0] = eat_array[diffs>0] # ate it\n",
    "    x_train[diffs<=0] = ignore_array[diffs<=0] # ignored it\n",
    "    \n",
    "    # get training labels\n",
    "    vals = np.random.normal(0, 1, batch_size)\n",
    "    y_train[vals>0] = -35\n",
    "    y_train[vals<=0] = 5\n",
    "    y_train[poisonous==0] = 5 # ate it (safe)\n",
    "    y_train[diffs <= 0] = 0 # if you ignored it\n",
    "\n",
    "    \n",
    "    # get regret for this episode\n",
    "    oracle_score = 5 * sum(poisonous==1) # optimal strategy eats all good and ignores all bad\n",
    "    agent_score = sum(y_train)\n",
    "    regret.append(oracle_score - agent_score)\n",
    "    \n",
    "    # backprop\n",
    "    outputs = eat_scores\n",
    "    outputs[diffs<=0] = ignore_scores[diffs<=0]\n",
    "    outputs = outputs.reshape((batch_size,))\n",
    "    loss = criterion(torch.tensor(outputs).cuda(), torch.tensor(y_train).cuda())\n",
    "    loss = Variable(loss, requires_grad = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not i % 100:\n",
    "        print('iteration:', i, 'regret:', oracle_score - agent_score, 'fit loss:', fit_loss.item(), 'kl loss:', KL_loss.item())\n",
    "\n",
    "print('time elapsed:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# cum_regrets = list(regret)\n",
    "# for i in range(1, len(cum_regrets)): cum_regrets[i] += cum_regrets[i-1]\n",
    "\n",
    "# fnn_cum_regrets1 = cum_regrets\n",
    "\n",
    "# plt.yscale('log')\n",
    "# plt.plot(fnn_cum_regrets[1:500], label='greedy')\n",
    "# plt.plot(bnn_cum_regrets[1:500], label='bnn')\n",
    "# plt.plot(fnn_cum_regrets5[1:500], label='1% greedy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 regret: 485.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 100 regret: 410.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /homes/awl27/testenv/lib/python3.5/site-packages/ipykernel_launcher.py:62: UserWarning:To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200 regret: 380.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 300 regret: 390.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 400 regret: 290.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 500 regret: 240.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 600 regret: 95.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 700 regret: 500.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 800 regret: 345.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 900 regret: 640.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1000 regret: 595.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1100 regret: 700.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1200 regret: 250.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1300 regret: 415.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1400 regret: 615.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1500 regret: 555.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1600 regret: 400.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1700 regret: 435.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1800 regret: 315.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 1900 regret: 385.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2000 regret: 275.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2100 regret: 375.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2200 regret: 615.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2300 regret: 195.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2400 regret: 330.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2500 regret: 505.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2600 regret: 130.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2700 regret: 475.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2800 regret: 370.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 2900 regret: 235.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3000 regret: 525.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3100 regret: 505.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3200 regret: 750.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3300 regret: 570.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3400 regret: 335.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3500 regret: 340.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3600 regret: 275.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3700 regret: 405.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3800 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 3900 regret: 355.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4000 regret: 515.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4100 regret: 325.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4200 regret: 375.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4300 regret: 470.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4400 regret: 340.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4500 regret: 480.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4600 regret: 630.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4700 regret: 555.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4800 regret: 390.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 4900 regret: 455.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5000 regret: 395.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5100 regret: 190.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5200 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5300 regret: 345.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5400 regret: 185.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5500 regret: 375.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5600 regret: 415.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5700 regret: 445.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5800 regret: 470.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 5900 regret: 410.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6000 regret: 630.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6100 regret: 500.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6200 regret: 425.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6300 regret: 445.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6400 regret: 330.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6500 regret: 280.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6600 regret: 410.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6700 regret: 150.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6800 regret: 355.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 6900 regret: 410.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7000 regret: 285.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7100 regret: 390.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7200 regret: 300.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7300 regret: 300.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7400 regret: 465.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7500 regret: 310.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7600 regret: 425.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7700 regret: 780.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7800 regret: 460.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 7900 regret: 575.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8000 regret: 430.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8100 regret: 250.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8200 regret: 185.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8300 regret: 320.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8400 regret: 315.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8500 regret: 495.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8600 regret: 465.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8700 regret: 410.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8800 regret: 325.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 8900 regret: 240.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9000 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9100 regret: 345.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9200 regret: 350.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9300 regret: 390.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9400 regret: 810.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9500 regret: 420.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9600 regret: 685.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9700 regret: 395.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9800 regret: 385.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "iteration: 9900 regret: 335.0 fit loss: 16785.8515625 kl loss: 13640.810546875\n",
      "time elapsed: 12.314108610153198\n"
     ]
    }
   ],
   "source": [
    "# train 5% greedy FNN\n",
    "t0 = time.time()\n",
    "\n",
    "#initialize net\n",
    "fnet = Net(119, nhid, 1)\n",
    "fnet.cuda()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(fnet.parameters(), lr=learning_rate)\n",
    "\n",
    "epsilon = 0.05\n",
    "num_episodes=10000\n",
    "batch_size=64\n",
    "regret = []\n",
    "eat = np.array([[1,0] for _ in range(batch_size)])\n",
    "ignore = np.array([[0,1] for _ in range(batch_size)])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # get 64 mushrooms\n",
    "    idx = np.random.randint(len(data), size=batch_size)\n",
    "    data_train = data[idx,:]\n",
    "    X, poisonous = data_train[:,1:], data_train[:,0]\n",
    "    \n",
    "    # get scores for eating and ignoring\n",
    "    eat_array = np.append(X, eat, axis=1)\n",
    "    eat_scores = fnet(torch.tensor(eat_array).float().cuda())\n",
    "    ignore_array = np.append(X, ignore, axis=1)\n",
    "    ignore_scores = fnet(torch.tensor(ignore_array).float().cuda())\n",
    "    \n",
    "    x_train, y_train = np.zeros(eat_array.shape), np.zeros(poisonous.shape)\n",
    "    \n",
    "    # get training vectors\n",
    "    diffs = (eat_scores.cpu().detach().numpy() - ignore_scores.cpu().detach().numpy()).reshape((batch_size,))\n",
    "    \n",
    "    # epsilon greedy behavior change to score differences\n",
    "    eps = np.random.uniform(0, 1, batch_size) # generate uniform vector\n",
    "    random_actions = np.random.normal(0, 1, batch_size)\n",
    "    random_actions[random_actions>0] = 1\n",
    "    random_actions[random_actions<=0] = -1\n",
    "    diffs[eps > 1 - epsilon] = random_actions[eps > 1 - epsilon]\n",
    "    \n",
    "    x_train[diffs>0] = eat_array[diffs>0] # ate it\n",
    "    x_train[diffs<=0] = ignore_array[diffs<=0] # ignored it\n",
    "    \n",
    "    # get training labels\n",
    "    vals = np.random.normal(0, 1, batch_size)\n",
    "    y_train[vals>0] = -35\n",
    "    y_train[vals<=0] = 5\n",
    "    y_train[poisonous==0] = 5 # ate it (safe)\n",
    "    y_train[diffs <= 0] = 0 # if you ignored it\n",
    "\n",
    "    \n",
    "    # get regret for this episode\n",
    "    oracle_score = 5 * sum(poisonous==1) # optimal strategy eats all good and ignores all bad\n",
    "    agent_score = sum(y_train)\n",
    "    regret.append(oracle_score - agent_score)\n",
    "    \n",
    "    # backprop\n",
    "    outputs = eat_scores\n",
    "    outputs[diffs<=0] = ignore_scores[diffs<=0]\n",
    "    outputs = outputs.reshape((batch_size,))\n",
    "    loss = criterion(torch.tensor(outputs).cuda(), torch.tensor(y_train).cuda())\n",
    "    loss = Variable(loss, requires_grad = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not i % 100:\n",
    "        print('iteration:', i, 'regret:', oracle_score - agent_score, 'fit loss:', fit_loss.item(), 'kl loss:', KL_loss.item())\n",
    "\n",
    "print('time elapsed:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff567523eb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# cum_regrets = list(regret)\n",
    "# for i in range(1, len(cum_regrets)): cum_regrets[i] += cum_regrets[i-1]\n",
    "\n",
    "# fnn_cum_regrets5 = cum_regrets\n",
    "# pickle.dump(fnn_cum_regrets, open('results/fnn_regret.pkl', 'wb'))\n",
    "# pickle.dump(fnn_cum_regrets1, open('results/fnn_regret1.pkl', 'wb'))\n",
    "# pickle.dump(fnn_cum_regrets5, open('results/fnn_regret5.pkl', 'wb'))\n",
    "# pickle.dump(bnn_cum_regrets, open('results/bnn_regret.pkl', 'wb'))\n",
    "\n",
    "fnn_cum_regrets = pickle.load(open('results/fnn_regret.pkl', 'rb'))\n",
    "fnn_cum_regrets1 = pickle.load(open('results/fnn_regret1.pkl', 'rb'))\n",
    "fnn_cum_regrets5 = pickle.load(open('results/fnn_regret5.pkl', 'rb'))\n",
    "bnn_cum_regrets = pickle.load(open('results/bnn_regret.pkl', 'rb'))\n",
    "\n",
    "plt.yscale('log')\n",
    "cutoff = 100\n",
    "plt.plot(fnn_cum_regrets5[1:cutoff], label='5% Greedy')\n",
    "plt.plot(fnn_cum_regrets1[1:cutoff], label='1% Greedy')\n",
    "plt.plot(fnn_cum_regrets[1:cutoff], label='Greedy')\n",
    "plt.plot(bnn_cum_regrets[1:cutoff], label='Bayes by Backprop')\n",
    "\n",
    "plt.title('Cumulative Regret Against Training Step')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.xlabel('Training Batch (64 Steps per Batch)')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
